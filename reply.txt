We would like to thank all the reviewers for their comments and suggestions. We
have revised our paper taking into account the reviewers suggestions. In the follow-
ing, we will discuss the revisions and address the questions raised by the reviewers.

Reply to referee 1:

>The definitions of task and behavior are then described in sec IV. I
>would suggest the authors to explain them together with the formula-
>tion. In addition, the description of task and behavior which is clear
>in the formulation becomes a little bit confusing in sec IV.

We moved the paragraph about the tasks from section IV at the end of the section
III, where the task function formalism is defined.

>In section IV, which are the limitations of the proposed strategy? In
>particular, the authors assume to know the model of the robot. I guess
>they mean the kinematic model, please clarify. How much can the
>strategy cope with the model uncertainties.

It is indeed the kinematic model of the robot that is assumed to be known. The
kinematic model is required to compute the jacobian and the nullspace projector
that are used in the recognition part (the jacobian is used to project the movement
in the task space, the nullspace projector is used to cancel a task). As a conse-
quence, uncertainties in the kinematic model will lead to wrong jacobian or wrong
nullification of a task. However, the influence of an uncertain model has not been
estimated in that works. This influence has to be tested in future works involving
human movements.

>In addition they assume that all the tasks that may appear in a motion
>are known. This looks like a strong assumption, which requires further
>explanation. In addition I would suggest the authors to better explain
>the task pool.

The recognition method is meant to be used with predictables movements. Un-
known tasks are not supposed to happen. This assumption allow the recognition
problem to be treated as a selection problem in a finite set. That finite set is materi-
alized by the task pool. The use of parallel tasks enlarge the movement expressivity
of the movement to be analyzed.

>Are there cases in which the strategy has low performance?

We encounter poor performances when the movements generated for the test con-
tains numerical errors: we tried to randomly build stack of tasks, but singularities
where often encountered resulting in movements that have no sense.

>In addition, the method is based on the observations coming from the
>robot. Are you planning to extend the method to also deal with obser-
>vations coming from different sources, apart from the same robot?

The final discussion mentions the use of the method on human motions. Some
works in biomechanics comforts us in a way that it has been showed that the re-
dundancy is handled by the central nervous system in a similar way that roboticians
handle redundancy for robots [1]. The consequence is that applying the recognition
method to human motion makes sense if behavior model (analytical or statistical)
of human motion are known.

>In section V, the differences between the left and right side in Fig.
>3-4-5 are not clear.

The differences between the left and right side are ambiguous on purpose. The
ambiguity is used to illustrate the precision of the recognition method.

>Why the threshold for the stop criterion is fixed and how do you select
>its value. Please make clear the tuning procedure.

The threshold is empirically set in order to be above numerical noise due to the
successive projections.

>In section VI, Fig. 15-16-18 are not so clear. Please try to distinguish
>the norm of the motion in different colors, instead of scale of green.

Caption of those figures where rewritten, Fig. 15 is used to illustrate the tasks cou-
pling: the quantity of motion of the right hand space is illustrated. This quantity of
motion involves all the degree of freedom of the kinematic chain from the free float
to the hand. That quantity of motion becomes almost null after the nullification of
two tasks: Right Grab and Com. It means that the motion of the kinematic chain
is explained by the execution of those tasks. Fig. 16 and 18 is used to illustrate the
decrease of the quantity of motion over the iterations of the recognition algorithm.

>During the discussion of the results the authors mention the value of
>r such as high or low. I would suggest the authors to define a range
>(with discussions) to describe a reasonable (or not) result.

Caracterizing r as ”high” or ”low” was a mistake. The value of the residues have
to be compared within each others. At the iteration i, the task fitting which has the
lowest residue involves the better task to select at that iteration.

Reply to referee 2:

>The approach requires strong assumptions that makes its practical
>application to real-world scenarios disputable (the experiment is on a
>real robot, but the underlying target application is not clear). Indeed,
>the system works only if the robot being observed shares the same
>pre-designed set of controllers and the same robot capabilities (same
>kinematic structure, etc.). It would thus not be possible to use the
>method (at least as it is presented in the paper) to recognize a move-
>ment executed by a human user. The observation must come from the
>same robot or another copy of the robot. In this second case, it is diffi-
>cult to find scenarios where two identical robots could not have access
>to each other’s states through non-human-like information exchange
>capabilities (why not using wireless communication capabilities?). If
>the authors have an application in mind that would counterbalance
>this remark, they should include it explicitly in the introduction of the
>paper as a motivation for the proposed system.

The motivation for the proposed system was not clear enough in the introduction
of the first version of the paper. The proposed method was intended to show that it
was possible to perform task recognition knowing how the motion is generated, and
in particular, how the redundancy is used to perform parallel tasks. The approach
can be seen as a reverse engineering method. The observation does not have to
come from the same robot but from a robot where the assumptions are valid. A
short paragraph has been added at the end of introduction to illustrate that point.
The experiments on the robot illustrate that the method is robust to sensor noise.

For the particular case of human motions, biomecanicals studies shows that the
redundancy can also be viewed as a task space and a null space as it is formulated
in the uncontrolled manifold hypothesis [2]. The degree of freedom of a human
can be separated in two classes: the controlled and the uncontrolled ones for a par-
ticular task. The uncontrolled manifold (UCM) can be compared to the nullspace
of the task jacobian, and the orthogonal manifold (ORT) can be compared to the
task jacobian. In the experiments performed in [1], considering a reaching task per-
formed by a human, it is shown that the variance of at the elbow becomes higher
in presence of an obstacle (this obstacle becomes a geometrical constraint for the
reaching task): in order to handle secondary objectives (obstacle avoidance), the
flexibility at the elbow level is used.

We did not detail the investigation about the application of the recognition
method to human motion, because we believe that it was out of scope for this
paper.

>Also, it is not discussed how the approach would scale up if a full
>database of movements is available. Exponential decrease movement
>behaviors are tested in the experiment: how do the authors plan to
>extend the system to movements that change depending on position
>of objects or state of the robot/environment (namely, motion control
>function with input parameters)?

Using the task function approach, the database of tasks is not so large. Typical
database would include 6D position tasks for an effector, center of mass regula-
tion, visual servoing, relative position between two parts of the robot, posture (full
body or not). Being able to compose a movement by executing tasks in parallel
(stacking the tasks) allow a great expressivity of movements and we believe that
this expressivity is enough for a robot.

Behaviors others than the exponential decrease can be used as a fitting criterion
as it is shown in the generic formulation (8).

>Also, recent robot controllers are very often defined by a variety
>of movement constraints, such as inequality/equality constraints, soft
>constraints (such as loose workspace areas to reach or to stay in), or
>conditional constraints (e.g. moving until something is detected). For
>which of those controller behaviors will the proposed approach be ap-
>plicable? The hypotheses are clearly stated, which is appreciable, but
>extending the discussion to clarify the reason why they are required
>would be relevant: is it an implementation detail or a limiting factor?
>Are there potential ways to relax some of these hypotheses? What are
>the underlying implications they bring?, etc.

At this moment, only equalities constraints can be handled. It is explained by the
assumption of a stack of tasks has to be constant during the observed movements.
The stack must involves tasks that begin and end at the same time. In order to
handle inequalities constraints (and also conditional constraints), it is necessary
to detect when constraints become inactive. The starting and ending time can be
added to the task fitting optimisation problem but this solution has not yet been
tested. The section describing the assumptions has been revised to reflect this ex-
plaination.

>The authors should also discuss how the redundant robot structure is
>important for the proposed approach. Does the approach strongly rely
>on the control redundancy to recognize sets of parallel tasks, or could
>this hypothesis be somehow relaxed if one wants to extend it to larger
>sets of tasks or humanoids with fewer joint articulations?

The redundancy is used to decouple the tasks executed in parallel, if there are
several tasks involved in the movement. We added this explanation in the overview
of the algorithm in section IV.

>Depending on the number of parallel tasks, compromises need to be
>adopted that do not always follow a strict hierarchy. Will the recogni-
>tion be possible in this situation (e.g. if the system is underactuated)?
>Will the order of nullspace projections have an influence?

The hierarchy is not considered during the recognition since all tasks involved in
the movements are completely fulfilled.

>In Section II, the authors point at the drawbacks of recognition meth-
>ods based on the pre-specification of a set of possible spaces onto
>which the data are projected to find the subspaces matching the ob-
>served data. In the next paragraph, they then contrast this methodol-
>ogy to their approach based on reverse control, which actually relies
>on very similar core principles and hypotheses. The authors should
>reframe their work with respect to other more classical recognition
>approaches in a more transparent way concerning this point.

The pre-specification of possible spaces used in previous works does not rely on
the method of generation of movements but rather on heuristics. This has been
precised in section II where we present the originality of our method.

>In the proposed experiment, the set of active tasks is constant during
>the motion. How would the system cope with parallel tasks of variable
>lengths activated sequentially? Could this be tested in simulation?

The method does not handle sequences of tasks. This is part of the future works:
sequences of stack of tasks will be considered. An instance of a stack of tasks
could be a state of a probabilistic automata that can be learned a priori for a specific
scenario.

>Does the stopping criterion threshold need to be adjusted depending
>on the number and type of tasks that are contained in the set of possible
>tasks?

The stopping criterion threshold need to be adjusted depending on the number of
tasks involved in the motion because this threshold is used to handle the numeric
noise introduced by the projection of the motion at each iteration and the noise
introduced by the sensors (for example the noise from the motion capture data).
This is now precised when we define a value of that threshold in the experiment
section V.C.

>How will the prevailing singularities affect recognition?

Numerical error will occurs in the neighborhood of the singularities, leading to
errors in the recognition. We do not study how much it affects the recognition
because when we generate a motion, we tried to avoid non consistent motion.

Additional references:
[1] J. Jacquier-Bret, N. Rezzoug, and P. Gorce, “Adaptation of joint flexibility
during a reach-to-grasp movement,” Motor Control, vol. 13, pp. 342–361, July
2009.
[2] J. P. Scholz and G. Schner, “The uncontrolled manifold concept: identifying
control variables for a functional task,” Experimental brain research, vol. 126,
pp. 289–306, June 1999.


