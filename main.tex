\documentclass[letterpaper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

%\setlength{\topmargin}{0.46in}


% The following packages can be found on http:\\www.ctan.org
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage[dvips]{graphicx} % for eps
\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{wasysym}
\usepackage{color}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{tensor}
\usepackage{leftidx}
\usepackage{subfigure}
\usepackage{supertabular}
%\renewcommand{\thesubfigure}{\thefigure.\arabic{subfigure}}
%\makeatletter
%\renewcommand{\p@subfigure}{}
%\renewcommand{\@thesubfigure}{\thesubfigure:\hskip\subfiglabelskip}
%\makeatother

%\usepackage[dvips, bookmarks=false, colorlinks=true, pdftitle={Hak-Humanoids2010}]{hyperref}
\input{./macros.tex}

\title{\LARGE \bf
Motion Analysis by Reverse Control for a Humanoid Robot
}

\author{Sovannara Hak, Nicolas Mansard, Olivier Stasse, Jean Paul Laumond% <-this % stops a space
  \thanks{7 av col Roche, F-31077 Toulouse, France, Universit\'e de Toulouse; UPS, INSA, INP,
    ISAE: {\tt\small sovannara.hak@laas.fr, nicolas.mansard@laas.fr, jpl@laas.fr}. Olivier
    Stasse, CNRS-AIST, JRL (Joint Robotics Laboratory), UMI 3218/CRT,
    Intelligent Systems Research Institute AIST Central 2, Umezono 1-1-1
    Tsukuba, Ibaraki 305-8568 Japan: {\tt\small olivier.stasse@aist.go.jp}}
  \thanks{This work was supported by a grant from the R-Blink Project, Contract
    ANR-08JCJC-0075-01.}  }

\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.75}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
%Statistical approaches are widely used for motion recognition.
%However, those statistical methods need to be applied in a \emph{suitable} space.
Efficient methods to perform motion recognition have been developed
using statistical tools. Those methods rely on primitives learning
in a \emph{suitable space}, for example, the latent space of the joint-angle and/or adequate task spaces.
Learned primitives are often sequential : a motion is segmented according to the time axis.
When working with a humanoid robot, a motion can be decomposed into
simultaneous sub-tasks. For example, in a waiter scenario,
the robot has to keep some plates horizontal with one of its arms, while placing a plate
on the table with its free hand.
Recognition can thus not be limited to one task per consecutive segment of 
time.
The method presented in this paper
takes advantage of the knowledge of what tasks the robot is able to do and how
the motion is generated from this set of known controllers, to perform a reverse engineering of an
observed motion. This analysis is intended to recognize simultaneous tasks that
have been used to generate a motion. The method relies
on the task-function formalism and the projection operation into the null space of a task to decouple
the controllers.
The approach is successfully applied on a real robot
to disambiguate motion in different scenarios where two motions look similar but have
different purposes.
\end{abstract}

\section{Introduction}
%\subsection{Motivation: what are you moving for?}

Current promising developments of service robotics stimulate the
research in human-robot interaction. In that context, understanding
robot actions from observation is a challenge per se. While an
intentional action originates at a planning level, its realization takes
place in the real world via motions. How to recognize an action from
observed motions? Defining methods to automatically recognize the goal
pursued by a robot performing a given motion is a critical issue. If we
consider mobile manipulators (e.g., PR2 robots),
there is a clear separation between navigation functions and
manipulation functions. The question of action recognition may be rather
simple. Similarly, a humanoid robot can be divided in two distinctive parts,
legs and upper body, which correspond to the navigation and manipulation
functions.
%\subsection{Problem statement: disambiguating motions in embodied actions.}
\begin{figure*}[t]
  \centering
  \makeatletter
  \renewcommand{\@thesubfigure}{Scenario \thesubfigure:\hskip\subfiglabelskip}
  \makeatother

  \subfigure[The global task \emph{Give me the ball} is decomposed into a
  sequence of sub-tasks \lbrack locate the ball\rbrack, \lbrack walk to the ball\rbrack, \lbrack grasp the
  ball\rbrack, \lbrack locate the operator\rbrack, \lbrack walk to the operator\rbrack, and \lbrack give the
  ball\rbrack. The motions \lbrack walk to\rbrack, \lbrack grasp\rbrack, \lbrack give\rbrack~appear as a sequence
  structuring the action.]{
  \makebox[\linewidth]{
  \begin{tabular}{c@{}c@{}c@{}c@{}c@{}}
    \includegraphics[height=2.4cm]{img/purpleBall1.ps}&
    \includegraphics[height=2.4cm]{img/purpleBall2.ps}&
    \includegraphics[height=2.4cm]{img/purpleBall3.ps}&
    \includegraphics[height=2.4cm]{img/purpleBall4.ps}&
    \includegraphics[height=2.4cm]{img/purpleBall5.ps}\\
  \end{tabular}
  \label{fig:introExample:purpleBall}
  }
  }
  \subfigure[To grasp the ball between its feet, the robot has to step
  away from the ball. In this experiment \emph{stepping away} is not a software
  module. It is an integral part of the embodied action \emph{grasping}]{
  \makebox[\linewidth]{
  \begin{tabular}{c@{}c@{}}
    \includegraphics[height=2cm]{img/graspFeet1.ps}&
    \includegraphics[height=2cm]{img/graspFeet2.ps}\\
  \end{tabular}
  \label{fig:introExample:graspFeet}
  }
  }
  \subfigure[To grasp the ball between in front of it (left), the robot
  reaches a posture where the left arm is used to maintain its balance. In
  the figure on the right, the robot performs two actions simultaneously:
  grasping a ball in front of it while grasping a ball behind (of course
  the ball behind has been intentionally placed at the end
  position of the left hand depicted on the left side). It
  is not possible to spot the difference between both postures. However,
  the question we address is: Is it possible to spot the difference
  between both \emph{motions}?]{
  \makebox[\linewidth]{
  \includegraphics[height=2.4cm]{img/spotDiff1H.ps}
  }
  \label{fig:introExample:spotDiff}
  }
  \caption{Introductory examples of embodied intelligence.}
  \label{fig:introExample}
\end{figure*}
For example, consider the \emph{Give me the purple ball} scenario~\cite{yoshida07}
performed by the humanoid robot HRP-2 at LAAS-CNRS as shown in Fig.~\ref{fig:introExample}. 
To reach the assigned objective, HRP-2 decomposes its mission into elementary sub-tasks,
each of them being adressed by a dedicated software module. For instance,
to reach the ball, the robot has to walk to the ball. \emph{Walking} appears as an
elementary action that is a resource to solve the problem that is processed by a
dedicated locomotion module. 

However, in the second scenario (Fig.~\ref{fig:introExample:graspFeet}),
HRP-2 has to grasp the ball that is located between its feet~\cite{kanoun10}. To reach the
objective, the robot has to step away from the ball and then grasp it. In this
experiment there is no dedicated module in charge of \emph{stepping}. \emph{Stepping} is
a direct consequence of \emph{grasping}. The grasping action is totally embedded in
the body, allowing the legs to naturally contribute to the action. Grasping
appears as an embodied action generating a complex motion. Finally Fig.~\ref{fig:introExample:spotDiff}
introduces the purpose of this paper. In the case on the left side, the robot
performs a single grasping task. In the case on the right side, the robot
performs two grasping tasks simultaneously. The ambiguity to distinguish both
cases comes from the role played by the left arm. In the first case, the left
arm contributes to the single grasping action by maintaining the balance of the
robot. In the second one, the left arm performs another grasping task. Both
motions are very similar. 

The works presented here tackles the problem of motion recognition and 
shows that it is possible to disambiguate both cases by focusing
the analysis of the motion in the task spaces and on
the behaviors of the controllers of the robot.

\section{Related work}
Recognition of motion is a problem that comes far away from robotics.
In the vision community, this problem is generally looked from the
unstructured motion point of view: no hypothesis are done
even on the shape or rigidness of the moving body.

However, informations are used from the environment and the recognition is mainly done from
the context. For example, salient points in time and position (looking at the 2D video
flow as a 3D function)~\cite{laptev05}. These points are learned from a database, then
matched during the demonstration. 
The structure of a moving multi rigid body is often supposed as known
and is used to estimate its pose. For example the estimation
of a humanoid pose trajectory in \cite{zordan03} is performed 
using optical motion-capture data to guide a known physical model.
In the following, we will consider that the motion of the rigid bodies is known.

%In the quest of robot autonomy, research and development in Robotics is
%dominated by the stimulating competition between abstract symbol manipulation and physical signal
%processing, between discrete data structures and continuous variables.
%Indeed finding a proper way to relate the discrete space of symbols and the continuous space
%of controllers is a challenge.\\
%%%
Statistics have been successfully applied
to action recognition and motion analysis~\cite{schaal03}.
Statistical tools are used to create symbols, and by extension, detect those
symbols in a motion. For example, a method for behavior-based control 
is proposed in~\cite{drumwright03, drumwright04}. Behaviors are defined 
as a motion symbol (e.g. jab, hook, elbow, shield and uppercut). 
The behaviors are modeled by learning from series of examples.
A dimensional reduction is then applied to have a significant
clusterization.  The recognition part is handled by a Bayesian classifier which
recognize a trajectory in joint or Cartesian space. The extension to the recognition
is to perform an imitation. This is performed by interpolating known examples to obtain
feasible trajectories. The introduction of Partially
Observable Markov Decision Process or Bayesian inference~\cite{pearl88} has
renewed the topic of action modeling~\cite{kaelbling98} in the last decade. Such
techniques and related ones are now applied to motor skill learning~\cite{peters08} in
general, and to motion segmentation~\cite{calinon10, inamura04} in particular. 
Generally speaking, the efficiency of statistics-based recognition is ruled by the quality of the dataset built
in the learning phase.
Several demonstrations for each particular cases are needed in order to extract
the invariants that will discriminate the tasks. The sparsity of the demonstrations
can also limits the efficiency of the recognition. Finally, the sets of demonstrations
have to be associated with the correct symbols, which are generally given to the learning algorithm.

Alternatively, recognition can be based on specific criteria that are a priori given 
to the system. In~\cite{nakaoka07}, only the robot trajectories
are used to distiguish between various phases of motion. A task is
a complete whole body motion within a temporal segment. The global motion
is a sequence of tasks. Each task has its own
parameters called \emph{skills parameters}. The task recognition method is decomposed in two steps: 
first, for each tasks, find all the temporal segments in the observed motion
corresponding to that task.
The second step is the estimation of the skill parameters for each segment.
Each task is detected by the analysis of a trajectory projected in a specific space.
For example, the stepping task is detected by analyzing the trajectory 
of a foot; a squatting task is detected by analyzing the vertical trajectory of the waist.
The criteria used for detection and the associated dedicated projection spaces
are ad-hoc, built manually for a particular motion that has to be imitated by the robot.
Similarly,~\cite{muhlig09} uses a set of specific spaces
in which the observed motion is projected. Ad-hoc criteria
%Observations of a movement from sensors (camera, or motion capture system)
are used to automatically choose the set of task spaces that will best 
represent and generalize a given movement, in order to focus
a learning technique into that new space. The criteria used for the task-space selection
are expressed by some score functions inspired from neuroscience:
the saliency of the object that is manipulated, a variance of the dimension
of a space during several demonstrations, and some heuristics that express that
uncomfortable or exhausting motions reveal the presence of a task. Those heuristics
address the problem of how to spot tasks that involve no motion. The method
presented add some higher level information to a purely statistical analysis and relies
on task spaces as an appropriate space to represent movement. However
the efficiency of the task space selection depends on the strength
of the heuristics.

The common approach of these last works are to project the observed motion in some specific reduced spaces,
where the recognition is easier. These spaces can be chosen arbitrary~\cite{nakaoka07}, 
automatically selected~\cite{muhlig09} or learned~\cite{peters08}. Similarly, in control,
smaller-size spaces are used to define the control objectives and modulate the robot behavior.
For example the task function approach~\cite{samson91} expresses generic control objectives in
given n-dimensional task spaces.
The approach has been extended to handle a hierarchical set of tasks~\cite{siciliano91, nakamura87}
using the redundancy of a system.
\medskip
%In this work the control law is obtained by a \emph{stack of tasks}
%(SoT)~\cite{mansard07} ie hierarchical inverse kinematics
%\cite{siciliano91}.\\                                       

%In this paper another point of view is taken: the control theory based
%one. Control theory constitutes the second corpus that accompanies
%robotics development. Originating from mechanics and applied
%mathematics, it focuses on robot motion control~\cite{murray94,
%siciliano10}. Among all the
%contributions on linear and non-linear systems, robot control theory has
%provided efficient concepts for motion generation. The research
%initiated by A. Li\'egeois~\cite{liegeois77} on redundant robots (i.e. robots that have
%more degrees of freedom than necessary to perform a given task), and
%then developed by Y. Nakamura~\cite{nakamura91}, B. Siciliano, J.J. Slotine~\cite{siciliano91} and O.
%Khatib~\cite{khatib87} introduce mathematical machinery based on linear algebra and
%numerical optimization that allows for clever ways to model the symbolic
%notion of \emph{task}~\cite{samson91}.\\

%Those \emph{tasks} are defined by their tasks spaces (eg. position of the hand
%in an arbitrary frame\cite{nakamura86a,khatib87}, or position of a visual feature in the image
%plane \cite{espiau92,hutchinson96a}), a reference behavior in those tasks spaces
%(eg. exponential decrease to zero) and by the differential link between
%the task space and the actuator space, typically the task Jacobian.
%Given a set of active task, the corresponding control law can be
%obtained by inverting the equation of motion of the robot. 

%In this work, the analyzed motion is not seen as a sequence of motion primitives but as a
%superimposition of controllers. The recognition of sequences
%is out of the scope of the paper.
%The task-function is generally used to generate a motion\cite{siciliano91, mansard07}, and
The originality of the method presented in this paper is to use the properties of the
task function to perform a task-recognition. The main idea is
to perform a reverse engineering of an observed motion, knowing the set of all possible tasks that can appear 
and using the control law in the task space as characteristic trajectories. 
Under the hypothesis that the motion has been generated by stacking a set of controllers,
the motion is processed in order to seek the known behaviors in each task spaces. 
We named this reverse engineering algorithm approach \emph{reverse control}.
While all the approaches presented above can only recognize non-simultaneous task to recognize,
we rely on the control redundancy principles to recognize sets of simultaneous tasks.
Projections of the motion in the already-detected orthogonal task spaces
ensure an efficient decoupling of the recognition of the tasks performed
by the robot.
%The next section briefly reviews the mechanism of the task function approach used
%throughout this paper.
%
%The control law associated to the task function approach can be decomposed in two components:
%a main command and a secondary command.
%The secondary command is computed so as to not interfere with the main command. This is classically achieved by
%projecting secondary commands into the nullspace of the main command. All degrees of freedom
%that are not used by the main command will be used to achieve other commands.
%In this work, instead of using the projector onto the nullspace of a task to generate a new control law that takes into
%%account some lower priorities commands, the projector is used on a joint angle trajectory
%to remove the part of the motion that corresponds to a detected task, removing
%ambiguity produced by combination of tasks. Furthermore, the recognition method is
%performed in each known task spaces in order to analyze trajectories in appropriate spaces.\\
%
%The next section sum up the basics notions related to the task-function
%formalism used throughout this paper. The proposed algorithm for the task recognition
%is detailed in section~\ref{section:algorithm}. Section~\ref{sec:simu} and~\ref{section:real}
%illustrates the method respectively in simulation and on the real robot followed
%by concluding remarks.

We introduce in Section~\ref{sec:sot} the basics of what the work presented here
relies on: the task function formalism. The proposed algorithm to perform the 
motion analysis is presented in Section~\ref{sec:detect}.
Finally, experimentations that validate the method are presented in 
simulation in Section~\ref{sec:simu} and on the real HRP-2 robot in Section~\ref{sec:real}.


\section{Tasks and stack of tasks}
\label{sec:sot}
The task-function framework~\cite{samson91} is an elegant approach to describe intuitively
sensor-based control objectives. The advantage is that expressing the control law
in the most suitable subspace with respect to a given objective simplifies its construction
as well as its execution since the subspace is generally closely linked to the sensors of
the robot. Based on the redundancy of the system, this
approach can be extended to consider a hierarchical set of
tasks~\cite{siciliano91}. Complex motion can then be composed from simple tasks seen as atomic bricks
of motion. This composition mode, along with the obvious composition by temporal
sequencing offers a real versatility: subparts of the motion can be used in
very different situations without redesigning them for each case.

In the following, we consider that the robot input is the velocity $\mbf{\dot{q}}$,
where $\mbf{q}$ is the robot configuration vector.                   
A task is defined by a vector space 
$\mbf{e}$ and by the reference behavior $\mbf{\dot{e}^*}$ to be
executed in the task space.  
The Jacobian of the task is noted
$\mbf{J}=\dpartial{\mbf{e}}{\mbf{q}}$. 
Various typical behaviour can be chosen for $\mbf{\dot{e}^*}$. Typically, we will
use in the following an exponential decrease, set by
\begin{equation}
  \mbf{\dot{e}^*} = -\lambda\mbf{e}
\end{equation}
where $\mbf{e} = \mbf{s} - \mbf{s^*}$ is the error between a current
observed value $\mbf{s}$ and its arbitrary reference $\mbf{s^*}$,
and $\lambda>0$ is the gain that tunes the rapidity of the regulation of $\mbf{e}$ to $0$.
For example, the observed feature can be a 3D position $\mbf{p}$ of one of
the robot's end-effectors, to be brought to a chosen position $\mbf{p^*}$.
In that case $\mbf{J} = \dpartial{\mbf{p}}{\mbf{q}}$ is the articular Jacobian of the robot.

The control law is given by the least-square solution~\cite{liegeois77}:
\begin{equation}
\dot{\mbf{q}} = \mbf{J}^+ \mbf{\dot{e}}^* + \mbf{P}\mbf{z}
\label{eq:ltsq}
\end{equation}

\noindent where $\mbf{J}^+$ is the least-square inverse of $\mbf{J}$,
$\mbf{P} = \mbf{I} - \mbf{J}^+ \mbf{J}$ is the projection operator onto the null space
of $\mbf{J}$ and $\mbf{z}$ is any secondary criterion. $\mbf{P}$ ensures
a decoupling of the task with respect to $\mbf{z}$. 
Using $\mbf{z}$ as a secondary input, the control can be extended
recursively to a set of \emph{n} tasks. Those \emph{n} tasks
are ordered by priority : task number 1 being the highest priority task,
and task number \emph{n}, the lowest priority,
$task_i$ should not disturb $task_j$ if $i>j$.
The recursive formulation of the control law is proposed by~\cite{siciliano91} :
\begin{equation}
\dot{\mbf{q}}_i = \dot{\mbf{q}}_{i-1} + (\mbf{J}_i \mbf{P}_{i-1}^{A})^+
(\dot{\mbf{e}}^*_i - \mbf{J}_i \dot{\mbf{q}}_{i-1}) , \ \ i = 1 \ldots n
\end{equation}
\noindent with $\dot{\mbf{q}}_0 = 0$ and $\mbf{P}_{i-1}^{A}$ is
the projector onto the null space of the augmented Jacobian
$\mbf{J}_i^A = (\mbf{J}_1, \ldots \mbf{J}_i)$. Joint velocities realizing all 
the tasks is $\dot{\mbf{q}}^* = \dot{\mbf{q}}_n$.
A complete implementation of this approach is proposed in~\cite{mansard07} under the
name \emph{Stack of Tasks} (SoT). 

\section{Motion analysis for task recognition} 
\label{sec:detect}
\label{section:algorithm}
In the previous section the classical widely-used task-function formalism was recalled.
Each task can be used to generate a common pattern of motion in various situation.
In that sense, a task is at the same time the controller that can
generate a motion, as well as a descriptor of the motion that is currently executed.
This descriptor is quasi symbolic, but comes also with additional parameters
that characterize the way it is executed: for example, for exponential-decrease tasks,
they come with the parameters $\lambda$ that characterize their rapidity. In this section,
we propose to use tasks as a set of descriptors to recognize a demonstrated motion,
by identifying the set of tasks that have been used to
generate the observed motion.
The identified task set can then be used to characterize the observed motion, for
example, to distinguish between similar-looking movements.

\subsection{Hypothesis}
It is assumed that the behavior model ($\mbf{e}$, $\mbf{\dot{e}^*}$, $\mbf{J}$) of every
possibly-used tasks is known. The model of the robot and all tasks that may appear in a 
motion are known. The set of models of possible tasks is called the \emph{task pool}.
The observed motion is supposed to have been generated using an unknown sub-set of those tasks.  
It is also assumed that all the tasks involved in the demonstration are compatible
in the sense of the projection $\mbf{P}$ defined in equation (\ref{eq:ltsq})
(no algorithmic singularities~\cite{chiaverini97}) and that the set
of active tasks is constant during the motion.
Finally, the observed motion is given through
joint trajectories. \footnote{The joint-angle trajectories can be recovered using a motion
capture system as shown in the remainder of this paper.}

\subsection{Overview}
\label{sec:alg1:selec}
The input of the algorithm is joint trajectories and
the task pool. The algorithm is iterative: at each iteration of the algorithm, the task that seems the
most relevant is selected. The selection of a task relies
on a curve-fitting score, obtained by projecting the joint-angle trajectory in each task space.
The projected trajectories are
compared to the theoretical trajectories which are characteristic
of the execution of a task.
The observed motion is then projected
onto the null-space of the selected task, cancelling the effect of the task in the observed motion.
Another iteration of selection-projection is then executed, on the projected motion.
%In order to iteratively recognize simultaneous tasks,
The algorithm stops when the original motion is cancelled, by the iterative projections.
\newcommand{\shOUTPUT}{\textbf{Output: }}
\newcommand{\shINPUT}{\textbf{Input: }}

\begin{algorithm}[t]
  \caption{Task selection algorithm}
  \label{alg:taskSelection}
\begin{algorithmic}[1]
  \STATE \shINPUT $\mathbf{\hat{\dot{q}}}(t)$
\STATE \shOUTPUT $activePool$
\STATE $\mathbf{P}\mathbf{\dot{q}}(t)\gets \mathbf{\hat{\dot{q}}}(t)$
\WHILE{$\int \Vert \mathbf{P}\mathbf{\dot{q}}(t) \Vert ^2 dt > \epsilon$ }
  \FOR{task $i = 1..n$}
    \STATE $r_i \gets \mathrm{taskFitting}(i, activePool)$
  \ENDFOR
  \STATE $i_{select} \gets \mathrm{argmin}(r_i)$
  \STATE $activePool.\mathrm{push}(i_{select})$
  \STATE $\mathbf{P}\mathbf{\dot{q}}(t) \gets \mathrm{projection}(i_{select}, \mathbf{P}\mathbf{\dot{q}}(t))$
\ENDWHILE
\end{algorithmic}
\end{algorithm}
The algorithm is showed in Alg.~\ref{alg:taskSelection}.
The joint velocity trajectory
of the observed motion is denoted $\mathbf{\hat{\dot{q}}}(t)$.
$\mbf{P\dot{q}}(t)$ denotes the successive projections of the reference
motion. Before the first iteration, $\mbf{P\dot{q}}(t)$ is set to the reference motion.
Then, each iteration projects it in the selected-task null space.
$r_i$ denotes the score of the cost function of the optimization. $activePool$ denotes
the set of tasks selected during the algorithm. 

If the observed motion is exactly generated by a SoT whose every task can be 
found in the task pool, the resulting trajectory $\mathbf{P\dot{q}}$ after projection
onto all the active-task spaces is null. However, in presence of noise (like when
acquiring motion through real sensors)
a residue is systematically obtained, which implies to use a threshold
as stop criteria: the loop ends when the residue is below the noise of
the acquisition chain. $\epsilon$ denotes the threshold
of the motion norm below which the algorithm stops.

The next subsection describes the two main functions of the algorithm.
The procedure $\mathrm{taskFitting}(i, activePool)$ handles the curve fitting
of the observed motion and the theoretical motion. The process is detailed in section~\ref{sec:alg2:proj}.
The procedure $\mathrm{projection}(i, \mathbf{\dot{q}}(t))$ computes the projection of
velocity onto the null space of the task $i$ and apply the projection to the motion.


\subsection{Projection of the motion}
%\subsubsection{General case}
In order to cancel a task $i$, the joint trajectories are projected onto the nullspace
of that task by multiplying it with the projector onto the nullspace of all tasks to cancel.
For every time $t$ of the motion time interval:
\begin{equation}
  \mbf{P\dot{q}}(t) \leftarrow \mbf{P^A_{i\mathrm{select}}}(t) \mbf{P\dot{q}}(t) 
  \label{eq:projection}
\end{equation}
with 
\begin{equation}
  \left\{
      \begin{array}{rl}
	\mbf{P^A_0}(t) &= \mbf{I}\\
	\mbf{P^A_i}(t) &= \mbf{P^A_{i-1}}(t) - (\mbf{J_i}(t)\mbf{P^A_{i-1}}(t))^+(\mbf{J_i}(t)\mbf{P^A_{i-1}}(t))\\
      \end{array}
    \right.
  \label{eq:projector}
\end{equation}
This operation will cancel the component of the motion that is independent with regard to the other tasks, 
and modify by the way the coupled component of the motion. The remaining motion is then analyzed to 
detect the potentially remaining tasks.

\medskip
%\subsubsection{Example with two tasks}
Considering the following example, the effect of the projections,
where a motion composed of two arbitrary tasks is detailed. 
The control law for a motion composed of two tasks $\mbf{e_1}$ and $\mbf{e_2}$ is given by:
\begin{equation}
  \mbf{\dot{q}} = \mbf{J_1^+}\mbf{\dot{e}_1^*} - (\mbf{J_2}\mbf{P_1})^+ (\mbf{\dot{e}_2^*} - \mbf{J_2}\mbf{J_1^+}\mbf{\dot{e}_1^*})
  \label{eq:controlLaw2Tasks}
\end{equation}

Multiplying~(\ref{eq:controlLaw2Tasks}) by $\mbf{P_1}$ cancels the motion in the Task 1 space:
\begin{equation}
  \mbf{P_1} \mbf{\dot{q}} = \mbf{P_1} \mbf{J_1^+}\mbf{\dot{e}_1^*} + \mbf{P_1} (\mbf{J_2}\mbf{P_1})^+ (\mbf{\dot{e}_2^*} - \mbf{J_2}\mbf{J_1^+}\mbf{\dot{e}_1^*})  
  \label{eq:cancel1_2:1}
\end{equation}
The first term is null by definition of $\mbf{P_1}$.
If one tries to observe the effect of the remaining motion $\mbf{P_i\dot{q}}$ in the Task 2 space after the nullification
of the Task 1 component, the result of the projection is
multiplied by the task Jacobian $\mbf{J_2}$:
\begin{equation}
  \mbf{J_2} \mbf{P_1} \mbf{\dot{q}} = \mbf{\dot{e}_2^*} +
  \mbf{J_2}  \mbf{J_1^+} \mbf{\dot{e}_1^*}  
  \label{eq:J2P1q}
\end{equation}
Since $\mbf{J_2}\mbf{P_1}(\mbf{J_2}\mbf{P_1})^+ = \mbf{I}$ by hypothesis.
The first term is the
independent component of the second task, and the second term
is the component coupled with Task 1. 
Therefore, the projection onto the nullspace of the task discovered at the 
first iteration will contain the coupling effect that has to be handled when trying to discover another task.
This coupling will be handled in the next section.
%First, we will show what are the effects of the cancellation of Task 1 and Task 2.
%Second, we will show what are the effects of the cancellation of the task 2 and the task 1. 

\subsection{Task fitting by optimization} \label{sec:alg2:proj}
The quantification of the relevance of a given task, with respect to
an observed motion, is achieved by applying a non linear least-square optimization
between the actual observed motion projected in the task space and the reference 
behavior of a task over the unknown parameters:
\begin{equation}
  \mbf{x}^* = \underset{\mbf{x}}\argmin \frac{\int \Vert \mbf{\hat{e}}(t) - \mbf{e}_\mbf{x}(t) \Vert^2 \mathrm{dt}}{\int \Vert \mbf{\hat{e}}(t) \Vert^2 \mathrm{dt}}
\label{optimProblem}
\end{equation}

\noindent where $\mbf{\hat{e}}(t)$ is the observed trajectory, $\mbf{e}_\mbf{x}(t)$ is the trajectory
generated by the model using the parameters $\mbf{x}$. 
The score will thus be the residue after trying to obtain the best correspondence with the given
model in the task space.

In some cases, the trajectory $\mbf{\hat{e}}$ can be observed directly. For example, if the task space 
is the 3D position of the hand, a direct observation is possible.
However, in general, a direct observation is not possible. For example,
the center of mass of the robot is difficult to observe directly. 
In addition, it is not possible to observe directly the effect of the successive projection. 
$\mbf{\hat{e}}$ is then obtained by the kinematic model and 
joint trajectories of the robot: $\mbf{J_i}\mbf{\hat{\dot{q}}}$.
However, proceeding so would lead to equation~(\ref{eq:J2P1q}), where the coupling with previously
selected tasks appears. The projected motion is then augmented with coupling compensation.
\begin{equation}
  \mbf{\hat{e}}(t) = \mbf{J}_i\mbf{P}_A \mbf{\dot{q}}(t) + \mbf{J}_i\mbf{J}_A^+\mbf{\dot{e}}_A(t)
  \label{eq:compensation}
\end{equation}
where $A$ is the set of tasks that have already been detected and $i$ is the task candidate.
In that equation, all the terms corresponding to $A$ are known since identified in the previous
iteration of the algorithm.
\medskip
%In order to cancel the second task, the projector onto the null-space of both 
%tasks $\mbf{P_{2, 1}}$ is computed, and multiplied with the new control law
%~(\ref{eq:cancel1_2:1}).
%\begin{equation}
%  \mbf{P_{2, 1}}\mbf{P_1} \mbf{\dot{q}} = \mbf{P_{2, 1}} (\mbf{J_2} \mbf{P_1})^+ \mbf{\dot{e_2}^*} +
%  \mbf{P_{2, 1}}(\mbf{J_2} \mbf{P_1})^+  \mbf{J_2}  \mbf{J_1^+} \mbf{\dot{e_1^*}} 
%  \label{eq:P2P1q}
%\end{equation}
%Then the obtained control law
%is null because $\mbf{P_{2, 1}} = \mbf{P_{1, 2}} \subset \mbf{P_2}$ and $\mbf{P_2}(\mbf{J_2} \mbf{P_1})^+ = 0$.
%
In practice, the observation is sampled and the integral is a sum.
The optimization problem~(\ref{optimProblem}) is a non linear problem. To solve
it numerically, the CFSQP solver has been used~\cite{lawrence97}.

%The task reference behavior $\mbf{e_x}$ can, for example, be defined by an exponential regulation.
%In that case, the characteristic trajectory in the task
%space can be obtained by analytical integration of the differential equation $\mbf{\dot{e}} = -\lambda\mbf{e}$ :
%it is an exponential decrease with three input parameters :
%
%\begin{equation}
%p_{\mbf x}(t) = x_1 \mathrm{e}^{(-x_2 t)} + x_3
%\label{eqTask}
%\end{equation}
%
%The vector of parameters $(x_1, x_2, x_3)$ has then a material interpretation:
%$x_3$ is the value at convergence ($x_3=s^*$). $x_1+x_3$ is the initial value ($x_1=s(0)-s^*$).
%And $x_2$ is the gain ($x_2=\lambda$).

\subsection{Order of the nullification}
%In all our experimentation, the second term was
%negligible. 
Consider the observed motion to have been generated by a two-stages SoT~(equation (\ref{eq:controlLaw2Tasks})).
We will prove that the 
order of nullification of the tasks is not important.
On one hand, the \emph{Task a} then \emph{Task b} is nullified, on the other, \emph{Task b} then \emph{Task a} is nullified.
                                            
\subsubsection{Removing \emph{Task a}, then \emph{Task b}}
Let us prove that the coupling between tasks is properly handled by the algorithm, by proving
that the same values can be observed whatever the order of the selection of the tasks,
and that the final projected trajectory is null.

At iteration 1, the observation in \emph{task space a} is directly $\mbf{J}_a\mbf{\hat{\dot{q}}} = \mbf{\hat{\dot{e}}}_a$.
In \emph{task space b}, it is:
\begin{align}
  \mbf{J}_b\mbf{\hat{\dot{q}}} &=  \mbf{J}_b \mbf{J}_a^+ \mbf{\hat{\dot{e}}}_a + 
  \underbrace{\mbf{J}_b(\mbf{J}_b \mbf{P}_a)^+}_{\mbf{I}} (\mbf{\hat{\dot{e}}}_b - \mbf{J}_b \mbf{J}_a^+\mbf{\hat{\dot{e}}}_a)\\
  &=  \mbf{\hat{\dot{e}}}_b
%  \mbf{J_2\dot{q}^*} = & \underbrace{\mbf{J_2}\mbf{P_1}(\mbf{J_2}\mbf{P_1})^+}_{\mbf{I}} + 
%  \underbrace{\mbf{J_2}\mbf{P_1}(\mbf{J_2}\mbf{P_1})^+ - \mbf{I}}_{0} \mbf{J_2J_1^+\dot{e}_1^*}\\
%  = &\mbf{\dot{e}_2}
  \label{eq:J2q}
\end{align}
The nullification of the original control law (\ref{eq:controlLaw2Tasks}) gives (\ref{eq:cancel1_2:1}),
involving a null motion when projected in \emph{task space a}. In \emph{task space b} it is
\begin{equation}
  \mbf{J}_b \mbf{P}_a \mbf{\hat{\dot{q}}}  = 
  \underbrace{\mbf{J}_b \mbf{P}_a(\mbf{J}_b \mbf{P}_a^+)}_{\mbf{I}}(\mbf{\hat{\dot{e}}}_b - \mbf{J}_b \mbf{J}_a^+ \mbf{\hat{\dot{e}}}_a)
  \label{eq:proof:J2P1q}
\end{equation}
The term $\mbf{J}_b \mbf{J}_a^+ \mbf{\hat{\dot{e}}}_a$ is known because the \emph{task a} has been selected
in the previous iteration. As a consequence, the trajectory $\mbf{\hat{\dot{e}}}_b$
can be retrieved from the joint trajectory observed $\mbf{\hat{\dot{q}}}$.

The observation is obtained in \emph{task space b} is independent from the projection in the 
nullspace of \emph{task a}.
\subsubsection{Removing \emph{Task b}, then \emph{Task a}} 
The observation in \emph{task space b} is directly $\mbf{J}_b \mbf{\hat{\dot{q}}} = \mbf{\hat{\dot{e}}}_b$.
If projecting the original motion~(\ref{eq:controlLaw2Tasks}) in the nullspace of \emph{task b} at the 
end of the first iteration, the resulting motion is:
\begin{equation}
  \mbf{P}_b \mbf{\hat{\dot{q}}} = \mbf{P}_b \mbf{J}_a^+\mbf{\hat{\dot{e}}}_a + 
  \underbrace{\mbf{P}_b (\mbf{J}_b \mbf{P}_a)^+ (\mbf{\hat{\dot{e}}}_b - \mbf{J}_b \mbf{J}_a^+ \mbf{\hat{e}}_a)}_{\mbf{0}}
  \label{eq:proof:P2q}
\end{equation}
The observation of the \emph{task a} is:
\begin{align}
  \mbf{J}_a \mbf{P}_b \mbf{\hat{\dot{q}}} &= \mbf{J}_a \mbf{P}_b \mbf{J}_1^+ \mbf{\hat{\dot{e}}}_a\\
  &= \mbf{J}_a ( \mbf{I} - \mbf{J}_b^+ \mbf{J}_b ) \mbf{J}_a^+ \mbf{\hat{\dot{e}}}_a\\
  &= \mbf{\dot{e}}_a - \mbf{J}_a \mbf{J}_b^+ \mbf{J}_b \mbf{J}_a^+ \mbf{\hat{\dot{e}}}_a
  \label{eq:proof:J1P2q}
\end{align}
%It can be shown 
%(appendix 1) 
%that $\mbf{e^*} = \mbf{\dot{e}_1^*}$, where $\mbf{e^*}$ is 
%defined as in equation~(\ref{eq:compensation})
\begin{equation}
  \mbf{\hat{\dot{e}}}_a = \mbf{J}_a \mbf{P}_b \mbf{\hat{\dot{q}}}(t) + \mbf{J}_a \mbf{J}_b^+\mbf{\hat{\dot{e}}}_b  
  \label{eq:J1P2q}
\end{equation}
Similarly to previous section, the control law after projection in the nullspace of \emph{task a} at the second iteration is null.
%This proves that detecting Task 1 after Task 2 is equivalent to detecting Task 2 after Task 1.
This proves that the same fitting is obtained independently of what task is detected first.
It proves simultaneously that the proper discovery of the tasks leads to a final null projected movement.
The proof can be extended using exactly the same arguments for a set of $n$ tasks.

\section{Results in simulation}
\label{sec:simu}
This section details a series of experimentation in simulation to validate the recognition algorithm.
Simulation allows us to emphasize the nominal behavior of the algorithm without 
any sensor noise.
The first experiment simply validates the projection of the motion~(section \ref{sec:prelimValid}). 
%Fig.~\ref{fig:snapshotXpqdot} and Fig.~\ref{fig:xp3Pqdot} illustrate the effect of the projections.
The second part compiles a set of experiments that validate the task recognition algorithm~(section \ref{sec:distinc}).
%Fig.~\ref{fig:XP2RFit} and Fig.~\ref{fig:XP2RLFit} shows how the fitting behaves. 
%Fig.~\ref{fig:RbeforeAfterProj} shows how a projected trajectory evolves after other projections.
For each experiment of that set, the task recognition algorithm proposed in this paper
is applied to two similar-looking motions:
the two motions have been artificially built to be ambiguous when 
compared to each other, in order to illustrate the 
efficiency of the algorithm regarding the precision of the recognition.
All motions involved in the experiments are summed up in Table~\ref{tab:motion}.
The description of the tasks used to build these motions are detailed below.

\subsection{Set-up}
The reference motions have been generated by using the model of the humanoid robot HRP-2 
having 30 actuated degrees of freedom plus six degrees of freedom on the
freeflyer. Every motions start from the half-sitting pose showed in Fig.~\ref{fig:halfSit}.
As classically done in inverse kinematics, the under-actuation of the freeflyer is resolved by constraining
the left foot to be on the ground. 
Fig.~\ref{fig:spotDiff1},
Fig.~\ref{fig:spotDiff2} and Fig.~\ref{fig:spotDiff3} show the final posture of the motions used in those
experiments.
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.3\linewidth]{img/halfSit.ps}
\end{center}
\caption{All reference motions start from a half-sitting pose.}
\label{fig:halfSit}
\end{figure}
\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.9\linewidth]{img/spotDiff.ps}
  \end{center}
  \caption{Left: The final position of the \emph{motion 2.a}; Right: The final position of the \emph{motion 2.b}.}
  \label{fig:spotDiff1}
\end{figure}
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.55\linewidth]{img/spotDiff2bis.ps}
\end{center}
\caption{Left: The final position of the \emph{motion 3.a}; Right: The final position of the \emph{motion 3.b}.}
\label{fig:spotDiff2}
\vspace{-3pt}
\end{figure}
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.66\linewidth]{img/spotDiff3.ps}
\end{center}
\caption{Left: The final position of the \emph{motion 4.a}; Right: The final position of the \emph{motion 4.b}.}
\label{fig:spotDiff3}
\end{figure}

The set of tasks considered in those experiments are :

\begin{itemize}
  \item \emph{Com} : the center of mass of the robot is constraint to maintain the static balance (3 DOF)
  \item \emph{Gaze} : the robot looks at one point in the Cartesian space (2 DOF)
  \item \emph{Twofeet}: both feet stay flat with regard to each other (6 DOF)
  \item \emph{Left/Right Grab} : either the left or right hand
of the robot reaches a point defined in the Cartesian space (3 DOF)
  \item \emph{Screw} : similar to the \emph{grab} task, but the desired position 
has to be reached with a defined orientation. (6 DOF)
  \item \emph{Head} : the head of the robot is constrained in position and orientation. (6 DOF)
  \item \emph{Chest} : the chest of the robot is constrained in orientation.  (3DOF)
\end{itemize}
\begin{table}[h]
  \centering
    \begin{tabular}{|c|c|c|}
      \hline
      & (a) & (b) \\
      \hline
      & &
      \\
      Motion 1 & 
	\begin{tabular}{|c|}
          \hline
          \emph{Right Grab}\\
          \hline
          \emph{Left Grab}\\
          \hline
          \emph{Gaze}\\
	  \hline
	  \emph{Com}\\
	  \hline
	  \emph{Twofeet}\\
	  \hline
	\end{tabular} & \\
      & &
      \\
      \hline
      & &
      \\
      Motion 2 & 
        \begin{tabular}{|c|}
          \hline
          \emph{Right Grab}\\
	  \hline
	  \emph{Com}\\
	  \hline
	  \emph{Twofeet}\\
	  \hline
	\end{tabular}  
    &
	\begin{tabular}{|c|}
	  \hline
	  \emph{Left Grab}\\
	  \hline
	  \emph{Right Grab}\\
	  \hline
	  \emph{Com}\\
	  \hline
	  \emph{Twofeet}\\
	  \hline
	\end{tabular}  
    \\
    & &
    \\
    \hline
    & &
    \\
    Motion 3 & 
        \begin{tabular}{|c|}
	  \hline
	  \emph{Gaze}\\
	  \hline
	  \emph{Right Grab}\\
	  \hline
	  \emph{Com}\\
	  \hline
	  \emph{Twofeet}\\
          \hline
        \end{tabular}  
    &
	\begin{tabular}{|c|}
	  \hline
	  \emph{Gaze}\\
	  \hline
	  \emph{Right Screw}\\
	  \hline
	  \emph{Com}\\
	  \hline
	  \emph{Twofeet}\\
	  \hline
	\end{tabular}  
    \\
    & &
    \\
    \hline
    & &
    \\
    Motion 4 & 
	\begin{tabular}{|c|}
	  \hline
	  \emph{Gaze}\\
	  \hline
	  \emph{Com}\\
	  \hline
	  \emph{Twofeet}\\
	  \hline
	\end{tabular}  
    &
	\begin{tabular}{|c|}
	  \hline
	  \emph{Left Screw}\\
	  \hline
	  \emph{Com}\\
	  \hline
	  \emph{Twofeet}\\
	  \hline
	\end{tabular} 
      \\
    & &
    \\
    \hline
    & &
    \\
    Motion 5 &
    \begin{tabular}{|c|}
      \hline
      \emph{Right Grab}\\
      \hline
      \emph{Gaze}\\
      \hline
      \emph{Com}\\
      \hline
      \emph{Twofeet}\\
      \hline
    \end{tabular}
    &
    \begin{tabular}{|c|}
      \hline
      \emph{Right Grab}\\
      \hline
      \emph{Chest}\\
      \hline
      \emph{Com}\\
      \hline
      \emph{Twofeet}\\
      \hline
    \end{tabular}
    \\
    & &
    \\
    \hline
  \end{tabular}
  \caption{Table of motions generated by their associated stack of tasks.}
  \label{tab:motion}
\end{table}

\subsection{Experiment 1 : Preliminary validation}
\label{sec:prelimValid}
In this experiment, a basic motion is generated. Then the recognition by fitting
and the termination condition of the algorithm are validated.
The reference motion is the \emph{motion 1.a} (see Table~\ref{tab:motion}).
The motion is given to the detection program
which selects the tasks that fit the most closely by the optimization.

Fig.~\ref{fig:snapshotXpqdot} shows snapshots of the original motion
and of the motions after successive projections
in the null spaces of the detected
tasks : \emph{Right Grab}, \emph{Com}, \emph{Gaze}, \emph{Twofeet} and \emph{Left Grab}.
Each projection cancels a part of the motion, and the robots motion becomes null when all
projections are applied,
which means that all relevant tasks have been detected.
\begin{figure*}[t]
\centering
\begin{tabular}{c@{}c@{}c@{}c@{}c@{}c@{}c}
(a)&
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot0_0.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot0_99.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot0_199.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot0_299.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot0_399.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot0_499.png.ps}}\\

(b)&
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot1_0.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot1_99.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot1_199.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot1_299.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot1_399.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot1_499.png.ps}}\\

(c)&
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot2_0.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot2_99.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot2_199.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot2_299.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot2_399.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot2_499.png.ps}}\\

(d)&
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot3_0.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot3_99.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot3_199.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot3_299.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot3_399.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot3_499.png.ps}}\\

(e)&
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot4_0.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot4_99.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot4_199.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot4_299.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot4_399.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot4_499.png.ps}}\\

(f)&
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot5_0.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot5_99.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot5_199.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot5_299.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot5_399.png.ps}} &
\parbox[c]{2.2cm}{\includegraphics[width=\linewidth]{img/Pqdot5_499.png.ps}}\\
\\
Iteration & 0 & 99 & 199 & 299 & 399 & 499\\
\end{tabular}
\caption{The motion generated by a stack of tasks involving :
\emph{right and left grab}, \emph{Com}, \emph{Gaze}, \emph{Twofeet} is represented in the row (a).
The remaining rows represent the successive projections of the motion in the null spaces of the tasks,
(b) Right Grab, (c) Com, (d) Gaze, (e) Twofeet, (f) Left Grab.}
\label{fig:snapshotXpqdot}
\end{figure*}
As we can see in the second line, the movement of the right hand is nullified. The cancellation of
the \emph{Com} from the third line is more difficult to perceive. On the forth line, the cancellation
of the head movement due to the gaze is very clear. On the fifth line, the cancellation of the task \emph{Twofeet} 
removes the compensation done with the right leg. Finally, as shown on the last line,
the cancellation of all tasks leads to a null motion.
Fig.~\ref{fig:xp3Pqdot} shows the evolution of the norm of the motion,
defined by the sum square of the joint-angle velocities of the robot.
\begin{figure}[t]
\begin{center}
%\includegraphics[height=0.9\linewidth, angle = -90]{img/PqdotNorms.ps}
\resizebox{.48\textwidth}{!} {
      \input{img/PqdotNormsProjection.tex}
    }
\end{center}
\caption{Evolution of the norm of the motion after successively projecting the motion in the
	tasks null spaces.}
\label{fig:xp3Pqdot}
\end{figure}
Each projection strictly decreases the norm of the velocity (ie the quantity of  movement). After
five projections, the movement is completely nullified, which confirms that all the active tasks have been
discovered. The algorithm can then stops.

\subsection{Experiment 2 : Distinction between two close motions}
\label{sec:distinc}
An interesting challenge in motion detection
is to make distinction between two motions
involving different tasks but that produce very similar joint trajectories.
Anthropomorphic algorithm would use the context to perform the disambiguation.
The work presented in this paper shows that the fitting criterion
is sufficient to disambiguate similar-looking motion. Three ambiguous pairs of movements
are presented to illustrate this capability.


\subsubsection{Reaching motions}
\label{sec:distinc1}
In this section, two motions are considered: \emph{motion 2.a} and \emph{motion 2.b}.
The first one is a far-reaching motion with the right hand.
The reaching motion of the right hand has an influence on the left arm through the \emph{Com} task:
to regulate its balance, the robot puts its left arm behind.
The second motion is the same reaching task for the right hand, added with a
second reaching task on the left hand. The desired position of the left
hand is artificially set to the final position of the left hand obtained at the first motion.

The final states of the robot for the two motions are shown in the Fig.~\ref{fig:introExample:spotDiff}.
A video showing those two motions on the HRP-2 is available\footnote{{http://homepages.laas.fr/shak/videos/}}.
The two motions look very similar, and it is very difficult to the human eye
to tell which motion involves a left and right hand task without the context.
In the first case, the motion of the left hand is due to the \emph{Com} and \emph{Right Hand} task, 
as it is a side effect to compensate the balance of the right hand.
In the second case, the motion of the left hand is decoupled,
since the left hand has been driven by its own goal.
However, in the proper task spaces, those motions appear clearly different.
Fig.~\ref{fig:XP2RFit} shows an example of the result of the task fitting of the 
\emph{Right Hand} and \emph{Left Hand} task applied to the \emph{motion 2.a}.
\begin{figure*}[t]
\centering
\begin{tabular*}{0.9\textwidth}{@{\extracolsep{\fill}}cc}
%\includegraphics[width=0.9\linewidth]{img/projRHNull.eps}
  \resizebox{.48\textwidth}{!} {
      \input{img/simu/2a/taskRhandNormInvJerr_0}
    }          &
  \resizebox{.48\textwidth}{!} {
      \input{img/simu/2a/taskLhandNormInvJerr_0}
    }\\
\end{tabular*}
\caption{Task fitting on the \emph{motion 2.a} on the right and the left hand tasks. Variable $r$ is the residue,
ie the distance between the two curves. The \emph{Right Hand} motion is properly fitted by
the task model, showing that the task is active. The \emph{Left Hand} is not fitted properly, since the task is 
not active. The two cases are easily distinguished by the residue value.} 
\label{fig:XP2RFit}
\end{figure*}
The residue of the optimization is high since the fitting is not possible on a 
trajectory that does not respect the model (in particular, the residue of the \emph{Left Hand} is 
much higher than the \emph{Right Hand}).
Both tasks are properly fitted, and will be detected.
The results of the detection algorithm
are showed in Table~\ref{tab:spotDiff1}.
\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Reference & Detected & $\int \Vert \dot{q}(t) \Vert ^2 dt$ & $\int \Vert P\dot{q}(t) \Vert ^2 dt$ \\
\hline
\begin{tabular}{c}
Com\\
Right Hand\\
Twofeet\\
\end{tabular}

&

\begin{tabular}{c}
Com\\
Right Hand\\
Twofeet\\
\end{tabular}

& 0.364398 & 0.00159355 \\
\hline
\begin{tabular}{c}
Com\\
Left Hand\\
Right Hand\\
Twofeet\\
\end{tabular}

&
\begin{tabular}{c}
Com\\
Left Hand\\
Right Hand\\
Twofeet\\
\end{tabular}

& 0.538329  & 0.0035343 \\
\hline
\end{tabular}
\caption{Results of the task selection algorithm from the analysis of the \emph{motion 2.a} and the \emph{motion 2.b}.}
\label{tab:spotDiff1}
\vspace{-20pt}
\end{table}
The first column shows the
tasks being used in the reference motion, the second column shows the tasks
selected by the algorithm, the third column shows the norm of the reference motion (quantity of motion
initially observed),
and the last column shows the norm of the reference motion projected onto the null space of
all selected tasks. The final quantity of movement
after projection is very low for both motions compared
to the threshold for the stop criterion $\int \Vert P\dot{q}(t) \Vert ^2 dt > \epsilon$
that is fixed to $\epsilon = 0.07$.

Finally Fig.~\ref{fig:RbeforeAfterProj} shows how the norm of joint-angle velocities corresponding to the tasks
\emph{Right Grab} and \emph{Left Grab} evolves after the projections.
\begin{figure*}[t]
\centering
  \subfigure[Motion 2.a]{
  \resizebox{.48\textwidth}{!} {
      \input{img/simu/2a/RbeforeAfterProj.tex}
    }
  \label{fig:RbeforeAfterProj:2a}
  }
  \subfigure[Motion 2.b]{
  \resizebox{.48\textwidth}{!} {
      \input{img/simu/2b/RbeforeAfterProj.tex}
  }
  \label{fig:RbeforeAfterProj:2b}
  }
  \caption{Evolution of the trajectory $\dot{\mbf{q}}$ projected in the spaces of
  the \emph{right grab} (in solid lines) and 
  \emph{left grab} (in dashed lines) after
  successive projection of the motion in the nullspace of the task \emph{Right Grab} and in the nullspace
  of the task \emph{Com}. In the \emph{motion 2.a}, a great part of the motion of the left arm is due to 
  the \emph{Com} task: removing the \emph{Com} task will cancel almost all motion in the left arm.
  In the \emph{motion 2.b}, the motion of the left arm is not only involved by the \emph{Com} task
  but mainly by the \emph{Left Grab} task. Removing the \emph{Com} task will only
  cancel a small part of the motion of the left arm.}
\label{fig:RbeforeAfterProj}
\end{figure*}
Projecting the \emph{motion 2.a} onto the null spaces of
the \emph{Right Grab} task will decrease its associated theoretical joint-angle velocities while leaving the
\emph{Left Grab} one unchanged. Fig.~\ref{fig:RbeforeAfterProj:2a} shows
that the theoretical joint-angle velocities associated to the
\emph{Left Grab} task is decreased after the projection of the motion in
the nullspace of the \emph{Com} task.  That explicits that
the left arm of the robot was moved by the \emph{Com} task.
By the way, the projection prevent the left task to be detected at any further iteration of
the algorithm.
But, when projecting the \emph{motion 2.b} into the nullspace of the \emph{Right Grab}
and \emph{Com} task the joint-angle velocities associated to the \emph{Left Grab} task
is still significant (Fig.~\ref{fig:RbeforeAfterProj:2b}). That means that the
\emph{Com} task has a little influence on the
motion of the left arm, and that motion on the left arm is due
to another task. Therefore, the task selection algorithm will keep looking
for the task that has controlled the left arm.
After the detection of the two (for the \emph{motion 2.a}) and three (for \emph{motion 2.b}) main tasks,
the norm of the last joint-angle velocities are not null, because 
the task selection algorithm has not finished and other tasks have not been selected yet.
The \emph{Twofeet} task is then detected but the projection is not displayed on the figure for the sake of clarity.
On the other hand Fig.~\ref{fig:XP2RLFit} shows the task fitting for the \emph{motion 2.b}.

\begin{figure*}[t]
\centering
\begin{tabular*}{0.9\textwidth}{@{\extracolsep{\fill}}cc}
%\includegraphics[width=0.9\linewidth]{img/projRHNull.eps}
  \resizebox{.48\textwidth}{!} {
      \input{img/simu/2b/taskRhandNormInvJerr_0}
    }                           &
  \resizebox{.48\textwidth}{!} {
      \input{img/simu/2b/taskLhandNormInvJerr_0}
    }\\
\end{tabular*}
\caption{Task fitting on the \emph{motion 2.b} on the right and the left hand tasks. Both motion of the right
and left hand are fitted by the model, with small residue: the tasks are detected.}
\label{fig:XP2RLFit}
\end{figure*}

\subsubsection{Grabing VS Screwing}
\label{sec:distinc2}
In this section, the two motions considered are the \emph{motion 3.a} and \emph{motion 3.b}.
Both motions share the same position target for the right hand. The only difference between
the two demonstrated motions is the
presence of an orientation constraint for the right hand in the \emph{motion 3.b}.
The final positions of those motions are shown in the Fig.~\ref{fig:spotDiff2}.
Table~\ref{tab:spotDiff2} shows the results of the task-selection algorithm which performed successfully.
\begin{table}[h]
  \centering
  \begin{tabular}{|c|c|c|c|}
    \hline
    Reference & Detected & $\int \Vert \dot{q}(t) \Vert ^2 dt$ & $\int \Vert P\dot{q}(t) \Vert ^2 dt$ \\
    \hline
    \begin{tabular}{c}
      Com\\
      Gaze\\
      Grab\\
      Twofeet\\
    \end{tabular}
    &
    \begin{tabular}{c}
      Com\\
      Gaze\\
      Grab\\
      Twofeet\\
    \end{tabular}
    & 0.838686 & 0.0132712 \\
    \hline
    \begin{tabular}{c}
      Com\\
      Gaze\\
      Screw\\
      Twofeet\\
    \end{tabular}
    &
    \begin{tabular}{c}
      Com\\
      Gaze\\
      Screw\\
      Twofeet\\
    \end{tabular}
    & 1.23008 & 0.0170635\\
    \hline
  \end{tabular}
  \caption{Results of the task selection algorithm from the analysis of the
  \emph{motion 3.a} and the \emph{motion 3.b}.}
  \label{tab:spotDiff2}
  \vspace{-3pt}
\end{table}
As in the previous experiments, the task \emph{Screw} can not be properly fitted on the trajectory
generated by the \emph{Grab} task alone. The \emph{Grab} task is then first selected in \emph{motion 3.a}.
The projection nullified the residual motion that could have led to a further wrong detection.
For \emph{motion 3.b}, both tasks \emph{Grab} and \emph{Screw} can be detected.
The residual motion after successive projections is finally very close to 0, which proved that
all tasks were properly detected.
%During the analysis of the \emph{motion 3.b}, the \emph{grabbing} task
%is selected as well since the grab task is
%actually a sub-task of the \emph{screwing} motion. Of course, the \emph{screwing} task is not selected
%in the analysis of the \emph{motion 3.a}.

\subsubsection{Screw VS Gaze}
\label{sec:distinc3}
The two motions considered are the \emph{motion 4.a} and \emph{motion 4.b}.
The \emph{motion 4.a} can be described by the following scenario :
an object is in front of the robot, and creates an occlusion in the vision of the robot.
To get rid of this occlusion, the robot can lean on its right. 
While the robot leans, the left hand is dragged by the chest as an involuntary side effect.
The position and orientation of that hand is recorded as a desired state of the
hand for the first \emph{motion 4.b} task.
In \emph{motion 4.b}, the gaze is uncontrolled. The motion of the head is,
that time, a side effect of the left hand movement.
The final states of the robot are shown in Fig.~\ref{fig:spotDiff3}
and the analysis results are summarized in Table~\ref{tab:spotDiff3}.
\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Reference & Detected & $\int \Vert \dot{q}(t) \Vert ^2 dt$ & $\int \Vert P\dot{q}(t) \Vert ^2 dt$ \\
\hline
\begin{tabular}{c}
Com\\
Gaze\\
Twofeet\\
\end{tabular}

&

\begin{tabular}{c}
Com\\
Gaze\\
Twofeet\\
\end{tabular}

& 0.534478 & 0.0545944 \\
\hline
\begin{tabular}{c}
Com\\
Screw\\
Twofeet\\
\end{tabular}

&
\begin{tabular}{c}
Com\\
Screw\\
Twofeet\\
\end{tabular}

& 0.558084 & 0.0023297\\
\hline
\end{tabular}
\caption{Results of the task selection algorithm from the analysis of the \emph{motion 4.a} and the \emph{motion 4.b}.}
\label{tab:spotDiff3}
\vspace{-12pt}
\end{table}

As previously, correct tasks are detected for each movement. The residue after projections on all detected
tasks are very small, proving that everything was properly detected and subtracted.

In conclusion, we have shown experimentally in this section that, without noise, the detection algorithm
performs perfectly, \emph{ie} detects exactly all the active tasks, without any  false detection,
and issues a neglectable residue of projected motion. The following section will consider
realistic cases of noisy signals acquired by real sensors.

\section{Experimentation on the robot}
\label{sec:real}
In this section, we will experimentally demonstrate the validity of the task-recognition algorithm
in realistic situation in presence of noise. This time, the reference motion 
is demonstrated by the real HRP-2 and observed 
using a motion-capture system. (Fig.~\ref{fig:hrp2Markers}).
As previously, the task recognition algorithm is applied to two pairs of similar looking motions.
First time, we will briefly explain how the motion-capture system is used to record the joint-angle 
trajectory. Then two pairs of motions are observed and provided to the recognition algorithm.

\subsection{Experimental setup}
A motion is executed with the \emph{SoT} on the HRP-2 robot equipped with markers on each
body (see Fig.~\ref{fig:hrp2Markers}).
\begin{figure}[t]
  \centering
  \begin{tabular}{cc}
    \includegraphics[height=0.7\linewidth]{img/hrp2Markers.ps} &
    \includegraphics[height=0.7\linewidth]{img/skel.ps} \\
  \end{tabular}
  \caption{Markers set and virtual skeleton of HRP2.}
  \label{fig:hrp2Markers}
\end{figure}
The motion-capture system used is composed of 10 digital cameras and record data
at 200Hz. 
The motion-capture system provides the trajectory of each body of the robot.
The data collected from those markers
are used to build a virtual skeleton that match the kinematic hierarchy of the robot (Fig.~\ref{fig:hrp2Markers}).
The result of this process is the 6D trajectories of all the bodies in space, without any joint constraints.

The analysis of the motion is performed on the joint-angle trajectories.
Therefore, a joint-angle trajectories have to be computed from the motion-capture data.
The joint-angle trajectories are computed as classically done by optimizing the distance
between the transformation matrix $\tensor[^{W}]{\mathbf{R}}{_{q_i}}(\mbf{q})$ from the 
robot origin to each joint of the robot and the measured
transformation matrix $\tensor[^{W}]{\mathbf{\hat{R}}}{_{q_i}}(t)$ using the robot's joint limits
as constraints. 
\begin{eqnarray}
  \mbf{\hat{q}}(t) =  & \underset{\mbf{q}}\argmin & \sum_i^n \Vert \tensor[^{W}]{\mathbf{\hat{R}}}{_{q_i}}(t) \ominus \tensor[^{W}]{\mathbf{R}}{_{q_i}}(\mbf{q}) \Vert ^2\\
    & \text{s.t.} & q_{i\mathrm{min}} \leq q_i \leq q_{i\mathrm{max}}, \; i = 1..n
  \label{mocapOpti}
\end{eqnarray}
where $\mbf{q}$ is the robot joint configuration vector,
$\ominus$ is the distance operator in SO(3), 
$\tensor[^{W}]{\mathbf{R}}{_{q_i}}(\mbf{q})$ is computed using the kinematic
model of the robot, and $\tensor[^{W}]{\mathbf{\hat{R}}}{_{q_i}}$ is obtained by:
\begin{equation}
  \tensor[^{W}]{\mathbf{\hat{R}}}{_{q_i}}(t) = \tensor[^{W}]{\mathbf{R}}{_{W_{C}}} \times \tensor[^{W_{C}}]{\mathbf{\hat{R}}}{_{M_i}}(t) \times \tensor[^{M_i}]{\mathbf{R}}{_{q_i}}    
  \label{mocapMatrix}
\end{equation}
where $\tensor[^{W}]{\mathbf{R}}{_{W_{C}}}$ and $\tensor[^{M_i}]{\mathbf{R}}{_{q_i}}$ are the constants
displacement due to the differences between an arbitrary skeleton model of the motion-capture and our kinematic model of the robot,
computed in a calibration step.
$\tensor[^{W_{C}}]{\mathbf{\hat{R}}}{_{M_i}}$
is the measured transformation matrix from the origin of the motion-capture reference frame
to the virtual body $i$. 

The obtained joint-angle trajectories are then used for the reverse-engineering task-recognition.\\

On the real robot, a flexibility is introduced after the ankle joint that
interferes with the motion at the very beginning as the acceleration
of the joint-angle increases quickly. The flexibility is not modeled 
neither in the control algorithm nor the detection method.
In order to bypass the influence of the flexibility, the motion analysis is done after the first 100ms
where the influence of the flexibility is predominant.
In the following two sections, we consider that the measures are directly the $\mbf{\hat{q}}$ trajectories
given by equation~(\ref{mocapMatrix}).

\subsection{Grabbing VS Maintaining balance}
This experiment corresponds to the one realized in simulation in section~\ref{sec:distinc1}:
the first motion is a motion of the left arm induced by the coupling between a far reaching of the right hand,
hand the balancing \emph{Com} task (\emph{motion 2.a}). The second motion is a reaching of both hands, built to be 
ambiguous with regard to the first motion~(\emph{motion 2.b} in Table~\ref{tab:motion}).
Table~\ref{tab:spotDiffReal1} shows the result of the identification algorithm for
the \emph{motion 2.a} and the \emph{motion 2.b} with a terminaison threshold set to $ \epsilon = 0.07$.
\begin{table}[h]
  \centering
  \begin{tabular}{|c|c|c|c|}
    \hline
    Reference & Detected & $\int \Vert \dot{q}(t) \Vert ^2 dt$ & $\int \Vert P\dot{q}(t) \Vert ^2 dt$ \\
    \hline
    \begin{tabular}{c}
      Com\\
      Right grab\\
      Twofeet\\
    \end{tabular}

    &

    \begin{tabular}{c}
      Com\\
      Right grab\\
      Twofeet\\
    \end{tabular}

    & 0.104835 & 0.0482885 \\
    \hline
    \begin{tabular}{c}
      Com\\
      Left grab\\
      Right grab\\
      Twofeet\\
    \end{tabular}

    &
    \begin{tabular}{c}
      Com\\
      Left grab\\
      Right grab\\
      Twofeet\\
    \end{tabular}

    & 0.142293  & 0.0541836 \\
    \hline
  \end{tabular}
  \caption{Results of the task selection algorithm from the analysis of the \emph{motion 2.a} and the \emph{motion 2.b} on the real robot.}
  \label{tab:spotDiffReal1}
  \vspace{-20pt}
\end{table}

For the \emph{motion 2.a}, the order of the task extraction is :
\emph{Right Grab}, \emph{Twofeet} and \emph{Com} task.
Whereas for the \emph{motion 2.b},
the extracted tasks are : \emph{Right Grab},  \emph{Com}, \emph{Twofeet} and \emph{Left Grab}.

Fig.~\ref{fig:exp1:headFit}-\ref{fig:exp1:taskLhand} show the
fitting at the first iteration of the algorithm for the \emph{Head} and \emph{Left Grab} tasks. As expected,
the fitting performs well only for the \emph{Left Grab} task in the \emph{motion 2.b} while it is rejected for
\emph{motion 2.a}.
\begin{figure*}[t]
  \centering
  \subfigure[Motion 2.a]{
  \resizebox{.48\textwidth}{!} {
  \input{img/realRobot/2a/taskHeadNormInvJerr_0.tex}
  }
  \label{fig:exp1:headFit:R}
  }
  \subfigure[Motion 2.b]{
  \resizebox{.48\textwidth}{!} {
  \input{img/realRobot/2b/taskHeadNormInvJerr_0.tex}
  }
  \label{fig:exp1:headFit:RL}
  }
  \caption{Fitting at the first iteration of the task selection algorithm
  for the \emph{head} task for \emph{motion 2.a} and \emph{motion 2.b} 
  For both motion, $r$ is high: the \emph{Head} task, not active, is not selected.}
  \label{fig:exp1:headFit}
\end{figure*}
\begin{figure*}[t]
  \centering
  \subfigure[Motion 2.a]{
  \resizebox{.48\textwidth}{!} {
  \input{img/realRobot/2a/taskLhandNormInvJerr_0.tex}
  }                           
  \label{fig:exp1:taskLhand:R}
  }
  \subfigure[Motion 2.b]{
  \resizebox{.48\textwidth}{!} {
  \input{img/realRobot/2b/taskLhandNormInvJerr_0.tex}
  }
  \label{fig:exp1:taskLhand:RL}
  }
  \caption{Fitting at the first iteration of the task selection algorithm
  for the \emph{Left Grab} task for the \emph{motion 2.a} and \emph{motion 2.b} 
  The \emph{Left Hand} task is not pertinent for \emph{motion 2.a}, but is selected for \emph{motion 2.b}.}
  \label{fig:exp1:taskLhand}
\end{figure*}

Although no \emph{Head} task is involved in the two motions, the head is not 
fixed in space due to the motion of the neck for balancing, and the chest and the legs
joints for all the active tasks.
In the \emph{motion 2.b}, the candidate \emph{Head} task involves less motion
than in the \emph{motion 2.a}. The reason is that when the opposite end effectors
of the robot are constrained, the chest is less used.
However, the task \emph{Head} is not kept as a candidate for any of the two motion (the residue $r$ is high).

The evolution of the motion projected into the \emph{Left Grab} and \emph{Right Grab}
spaces after the successive projections onto the automatically-selected nullspaces
are shown in figure Fig.~\ref{fig:exp1:Evolution2L}. For the \emph{motion 2.a},
the motion of the left arm is cancelled after removing the \emph{Com} task.
\begin{figure*}[t]
  \centering
  \subfigure[Motion 2.a]{
  \resizebox{.48\textwidth}{!} {
  \input{img/realRobot/2a/RevolutionLH.tex}
  }                           
  \label{fig:exp1:Evolution2L:a}
  }
  \subfigure[Motion 2.b]{
  \resizebox{.48\textwidth}{!} {
  \input{img/realRobot/2b/RevolutionLH.tex}
  }
  \label{fig:exp1:Evolution2L:b}
  }
  \caption{Norm of motions associated to the \emph{Left Grab} task (\emph{ie} motion of the left hand).
  The motion is cancelled after removing the motion due to the \emph{Com} task
  in \emph{motion 2.a}. In that case, it means that the left hand motion was due to the \emph{Com} task.
  Whereas in \emph{motion 2.b}, the motion of the left hand is not cancelled after
  removing the \emph{Com} task, but is cancelled after removing
  the \emph{Left Grab} task : the motion of the left hand was due
  to the \emph{Left Grab} task.}
  \label{fig:exp1:Evolution2L}
\end{figure*}
For the \emph{motion 2.b} the motion of the left arm is
not cancelled by the \emph{Com} projection, but is cancelled after
removing the \emph{Left Grab} task.
Fig.~\ref{fig:exp1:Evolution2R} shows how the motion of the right arm evolves when removing
motions due to a task. It can be seen that the right arm moves because of the \emph{Right Grab} and
\emph{Com} task.
\begin{figure*}[t]
  \centering
  \subfigure[Motion 2.a]{
  \resizebox{.48\textwidth}{!} {
  \input{img/realRobot/2a/RevolutionRH.tex}
  }                           
  \label{fig:exp1:Evolution2R:a}
  }
  \subfigure[Motion 2.b]{
  \resizebox{.48\textwidth}{!} {
  \input{img/realRobot/2b/RevolutionRH.tex}
  }
  \label{fig:exp1:Evolution2R:b}
  }
  \caption{Motion in the right hand space after successive projection.
  The motion of the right arm is a consequence of the  
  \emph{Right Grab} and \emph{Com} tasks, as explicited by the fact
  that the motion of the right arm is cancelled
  after the removal of the \emph{Right Grab} and \emph{Com} component of the motions.}
  \label{fig:exp1:Evolution2R}
\end{figure*}
Fig.~\ref{fig:exp1:PqdotNorms} shows the evolution of the quantity of motion after each task selection
on the two motions. The quantity decreases after successive projections, until the remaining motion 
is mainly sensor noise.
\begin{figure*}[t]
  \centering
  \subfigure[Motion 2.a]{
  \resizebox{.48\textwidth}{!} {
    \input{img/realRobot/2a/PqdotNormsR}
  }
  \label{fig:exp1:PqdotNormsR}
  }
  \subfigure[Motion 2.b]{
  \resizebox{.48\textwidth}{!} {
    \input{img/realRobot/2b/PqdotNormsRL}
  }
\label{fig:exp1:PqdotNormsRL}
}
\caption{Evolution of the norm of the motion after successively projecting the motion in the tasks null space for
the \emph{motion 2.a} and the \emph{motion 2.b}.}
\label{fig:exp1:PqdotNorms}
\end{figure*}

\subsection{Gaze VS Chest}
The considered motion are the \emph{motion 5.a} and \emph{motion 5.b}.
In the first case, the robot is looking at its right hand while grabbing something.
In the second case, the robot is grabbing something with its right hand while
maintaining its chest in its current orientation. Fig.~\ref{fig:motion5}
shows the final posture of the robot for those motions.
\begin{figure}[t]
  \centering
  \begin{tabular}{cc}
    \includegraphics[width=0.45\linewidth]{img/realRobot/5a/5aFinal1.ps} &
    \includegraphics[width=0.45\linewidth]{img/realRobot/5b/5bFinal1.ps} \\
  \end{tabular}
  \caption{Final position for \emph{motion 5.a} and \emph{motion 5.b}.}
  \label{fig:motion5}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
Table~\ref{tab:result5} summarizes the results of the task-selection algorithm.
Despite noise, the active tasks are detected. The two motions are
properly disambiguated. Finally, all tasks are properly discovered, as proved by the 
very small residue.
\begin{table}[h]
  \centering
  \begin{tabular}{|c|c|c|c|}
    \hline
    Reference & Detected & $\int \Vert \dot{q}(t) \Vert ^2 dt$ & $\int \Vert P\dot{q}(t) \Vert ^2 dt$ \\
    \hline
    \begin{tabular}{c}
      Com\\
      Gaze\\
      Right grab\\
      Twofeet\\
    \end{tabular}

    &

    \begin{tabular}{c}
      Com\\
      Gaze\\
      Right grab\\
      Twofeet\\
    \end{tabular}

    & 0.320001 & 0.0785168 \\
    \hline
    \begin{tabular}{c}
      Chest\\
      Com\\
      Right grab\\
      Twofeet\\
    \end{tabular}

    &
    \begin{tabular}{c}
      Chest\\
      Com\\
      Right grab\\
      Twofeet\\
    \end{tabular}

    & 0.216742 & 0.0516134 \\
    \hline
  \end{tabular}
  \caption{Results of the task selection algorithm from the analysis of the \emph{motion 5.a} and the \emph{motion 5.b} on the real robot.}
  \label{tab:result5}
\end{table}

Fig.~\ref{fig:exp6:PqdotNorms5} shows the evolution of the norm of the motions after the projection
into the null space of the tasks : \emph{Right Grab},  \emph{Gaze}, \emph{Twofeet} and \emph{Com} for
the \emph{motion 5.a}. And \emph{Right Grab}, \emph{Twofeet}, \emph{Com} and \emph{Chest} for the
\emph{motion 5.b}.
\begin{figure*}[t]
  \centering
  \subfigure[Motion 5.a]{
  \resizebox{.48\textwidth}{!} {
    \input{img/realRobot/5a/PqdotNorms5a}
  }
  }
  \subfigure[Motion 5.b]{
  \resizebox{.48\textwidth}{!} {
    \input{img/realRobot/5b/PqdotNorms5b}
  }
}
\caption{Evolution of the norm of the motion after successively projecting the motion in the tasks null space for
the \emph{motion 5.a} and the \emph{motion 5.b}.}
\label{fig:exp6:PqdotNorms5}
\end{figure*}
As in the previous experiments, the projection decreases strictly the quantity of movement
at each algorithm iteration. The quantity of movement finally remaining at the end of 
the algorithm loop is clearly mainly due to noise.

\section{Conclusion}
This article described a method to identify what tasks are composing an observed movement.
The analysis is driven by the knowledge
of what a task is and how it behaves (for example with exponential decrease). 
The analyzed movement is supposed to be generated by a set of 
controllers belonging to a known pool of tasks. 
The task recognition problem is then tackled by a reverse engineering of the motion.
The observed trajectory is analyzed
in each known task space to decide which ones are used.
The method does not rely on the nature of the behavior of a task. Therefore
any control laws used to generate a motion can be similarly
used to characterize an observed movement.

The method has been successfully applied in different
scenarios to discriminate similar-looking motions. Those motions were built to be especially
ambiguous in order to illustrate the efficiency of the method.

The experiments were limited to the analysis of robotic movement. In particular,
the experiments only considers exponential decrease as behaviors.
However, it could also be possible to handle other task models, for example, minimum jerk,
in order to match a human reaching trajectories.

\bibliographystyle{unsrt}
\bibliography{biblio}

\end{document}
