\documentclass[letterpaper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

%\setlength{\topmargin}{0.46in}


% The following packages can be found on http:\\www.ctan.org
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage[dvips]{graphicx} % for eps
\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{wasysym}
\usepackage{color}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{tensor}
\usepackage{leftidx}
\usepackage{subfigure}
\usepackage{supertabular}
%\renewcommand{\thesubfigure}{\thefigure.\arabic{subfigure}}
%\makeatletter
%\renewcommand{\p@subfigure}{}
%\renewcommand{\@thesubfigure}{\thesubfigure:\hskip\subfiglabelskip}
%\makeatother

%\usepackage[dvips, bookmarks=false, colorlinks=true, pdftitle={Hak-Humanoids2010}]{hyperref}
\input{./macros.tex}

\title{\LARGE \bf
Reverse Engineering the Motion to Perform Task Recognition\\ and Disambiguation on Humanoid Robot.
}

%\author{Sovannara Hak, Nicolas Mansard, Jean-Paul Laumond, Olivier Stasse% <-this % stops a space
  %\thanks{7 av col Roche, F-31077 Toulouse, France, Universit\'e de Toulouse; UPS, INSA, INP,
  %  ISAE: {\tt\small sovannara.hak@laas.fr, nicolas.mansard@laas.fr, jpl@laas.fr}. Olivier
  %  Stasse, CNRS-AIST, JRL (Joint Robotics Laboratory), UMI 3218/CRT,
  %  Intelligent Systems Research Institute AIST Central 2, Umezono 1-1-1
  %  Tsukuba, Ibaraki 305-8568 Japan: {\tt\small olivier.stasse@aist.go.jp}}
  %\thanks{This work was supported by a grant from the R-Blink Project, Contract
  %  ANR-08JCJC-0075-01.}  }


\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
%Statistical approaches are widely used for motion recognition.
%However, those statistical methods need to be applied in a \emph{suitable} space.
Efficient methods to perform motion recognition have been developped
using statistical tools. Those methods rely on primitives learning
in a \emph{suitable space}, for example the latent space of the joint angle and/or the task space.
The learned primitives are sequential : a motion is segmented according to the time axis.
When working with a humanoid robot, a motion can be decomposed into
independant tasks for example in a waiter scenario,
the robot has to keep some plates horizontal with one of his arms, while placing a plate
on the table with his free hand.
In order to be able to recognize each combination of those kind of independant task,
the statistical approaches need to learn the primitives from the demonstrations of each
combinations of tasks.
%Being able to explain the motion makes sense when working with a humanoid robot for example,
%because the effectors of that kind of robot can be used to achieve various goal in
%a meaning point of view. The robot can move its hand, in order to grasp
%an object or to maintain balance.
The method presented here
takes advantage of the knowledge of what tasks the robot is able to do and how
the motion is generated from a known set of controllers to perform a reverse engineering of an
observed motion. This analysis is intended to recognize the simultaneous tasks that
have been used to generate the motion. The method relies
on the task-function formalism and the projection operation into the null space of a task to decouple
the controllers.
The approach is successfully applied in simulation and on the real robot
to different scenarios where two motions look similar and have
different purposes.
\end{abstract}

\section{Introduction}
\subsection{Motivation: what are you moving for?}

The current promising development of service robotics stimulates the
research in human-robot interaction. In that context, understanding
robot actions from observation is a challenge per se. While the
intentional action originates at a planning level, its realization takes
place in the real world via motions. How to recognize an action from
observed motions? Defining methods to automatically recognize the goal
pursued by a robot performing a given motion is a critical issue. If we
consider mobile manipulators (e.g., PR2 robots\footnote{{http://www.willowgarage.com/pages/pr2/overview}})
there is a clear separation between navigation functions and
manipulation functions. The question of action recognition may be rather
simple. This may be not the case for humanoid robots whose
anthropomorphic shape entirely embodies complex actions. 

\subsection{Problem statement: disambiguating motions in embodied actions.}
\begin{figure*}[t]
  \centering
  \makeatletter
  \renewcommand{\@thesubfigure}{Scenario \thesubfigure:\hskip\subfiglabelskip}
  \makeatother

  \subfigure[The global task \emph{Give me the ball} is decomposed into a
  sequence of sub-tasks \lbrack locate the ball\rbrack, \lbrack walk to the ball\rbrack, \lbrack grasp the
  ball\rbrack, \lbrack locate the operator\rbrack, \lbrack walk to the operator\rbrack, and \lbrack give the
  ball\rbrack. The motions \lbrack walk to\rbrack, \lbrack grasp\rbrack, \lbrack give\rbrack~appear as a sequence
  structuring the action.]{
  \makebox[\linewidth]{
  \begin{tabular}{c@{}c@{}c@{}c@{}c@{}}
    \includegraphics[height=2.4cm]{img/purpleBall1.ps}&
    \includegraphics[height=2.4cm]{img/purpleBall2.ps}&
    \includegraphics[height=2.4cm]{img/purpleBall3.ps}&
    \includegraphics[height=2.4cm]{img/purpleBall4.ps}&
    \includegraphics[height=2.4cm]{img/purpleBall5.ps}\\
  \end{tabular}
  \label{fig:introExample:purpleBall}
  }
  }
  \subfigure[To grasp the ball between its feet, the robot has to step
  away from the ball. In this experiment \emph{stepping away} is not a software
  module. It is an integral part of the embodied action \emph{grasping}]{
  \makebox[\linewidth]{
  \begin{tabular}{c@{}c@{}}
    \includegraphics[height=2cm]{img/graspFeet1.ps}&
    \includegraphics[height=2cm]{img/graspFeet2.ps}\\
  \end{tabular}
  \label{fig:introExample:graspFeet}
  }
  }
  \subfigure[To grasp the ball between in front of it (left), the robot
  reaches a posture where the left arm is used to maintain its balance. In
  the figure on the right, the robot performs two actions simultaneously:
  grasping a ball in front of it while grasping a ball behind (of course
  the ball behind has been intentionally placed at the end
  position of the left hand depicted on the left side). It
  is not possible to spot the difference between both postures. However,
  the question we address is: Is it possible to spot the difference
  between both \emph{motions}?]{
  \makebox[\linewidth]{
  \includegraphics[height=2.4cm]{img/spotDiff1H.ps}
  }
  \label{fig:introExample:spotDiff}
  }
  \caption{Introductory examples of embodied intelligence.}
  \label{fig:introExample}
\end{figure*}

As an introductory example, let us consider the three actions performed by the
humanoid robot HRP-2 at LAAS-CNRS (Fig.~\ref{fig:introExample}). In the first scenario the robot
answers a single order: \emph{Give me the purple ball}~\cite{yoshida07}. To reach the assigned
objective, HRP-2 decomposes its task into elementary sub-tasks
(Fig.~\ref{fig:introExample:purpleBall}). A dedicated software module addresses each
sub-task. For instance,
to reach the ball, the robot has to walk to the ball. \emph{Walking} appears as an
elementary action that is a resource to solve the problem. It is processed by a
dedicated locomotion module. In the second scenario (Fig.~\ref{fig:introExample:graspFeet}),
HRP-2 has to grasp the ball that is located between its feet. To reach the
objective, the robot has to step away from the ball and then grasp it. In this
experiment~\cite{kanoun10} there is no dedicated module in charge of \emph{stepping}. \emph{Stepping} is
a direct consequence of \emph{grasping}. The grasping action is totally embedded in
the body, allowing the legs to naturally contribute to the action. Grasping
appears as an embodied action generating a complex motion. Finally Fig.~\ref{fig:introExample:spotDiff}
introduces the purpose of this paper. In the case on the left side, the robot
performs a single grasping task. In the case on the right side, the robot
performs two grasping tasks simultaneously. The ambiguity to distinguish both
cases comes from the role played by the left arm. In the first case, the left
arm contributes to the single grasping action by maintaining the balance of the
robot. In the second one, the left arm performs another grasping task. Both
motions are very similar. 

The purpose of the paper is to show that it is possible to disambiguate both
cases.

\section{Related work}
In the quest of robot autonomy, research and development in Robotics is
dominated by the stimulating competition between abstract symbol manipulation and physical signal
processing, between discrete data structures and continuous variables.
Indeed finding a proper way to relate the discrete space of symbols and the continuous space
of controllers is a challenge.\\

%%%
In the task recognition method presented in~\cite{nakaoka07}, a task is
a complete motion within a temporal segment and each tasks have their own
parameters called \emph{skills parameters}. Indeed, a global motion is a sequence of
tasks. The considered The task recognition method is decomposed in two step: 
first, for each tasks, find all the temporal segments corresponding to that task.
The second step is the estimation of the skill parameters for each segment.
Each task are detected by the analysis of the trajectory in a specific space:
for example the stepping task is detected by analyzing the trajectory  of a foot,
and a squatting task is detected by analyzing the vertical trajectory of the waist.
This is similar to the method introduced in this paper: each tasks are described in a particular
space, therefore the best way to detect those task is to analyze
those particular space.

%The pioneering researches addressing the possible autonomy of machines
%start in the seventies with the mobile robot Shakey: Fikes and Nilsson~\cite{fikes71}
%were the first researchers to consider that computational logic can be
%applied to machines to provide them with decision-making capabilities.
%This paved the route to consider robotics as a way to stimulate research
%in the, then nascent, Artificial Intelligence. Starting from logical
%reasoning~\cite{ghallab04} the approach imposes a top-down perspective (from symbol to
%signal) of system intelligence with the famous paradigm
%\emph{perception-decision-action}. During the eighties, under the impulsion
%of R. Brooks~\cite{brooks86}, a new line of research independently developed the idea
%that intelligence \emph{emerges} from a bottom-up organization of behaviors.
%This gave rise to the so-called bio-inspired robotics. All these
%approaches can be viewed as dominated by computer science at large. They
%aim at being generic. They address mobile robots as well as articulated
%mechanical systems. They tend to be independent from the mechanical
%dimensions of the system.\\
%
Statistics constitute another corpus that has been successfully applied
to action recognition motion analysis as it can be inferred 
from~\cite{schaal03} which quickly reviews the statistical and mathematical 
tools used to tackle motion imitation problems (which includes the recognition problem).
Statistical tools are used to create symbols, and by extension, detect those
symbols in a motion. For example, a method for behavior-based control 
is proposed in~\cite{drumwright03, drumwright04}. Those behaviors are defined 
as a motion symbol (e.g. jab, hook, elbow, shield and uppercut). 
The behaviors are modeled by learning from joint trajectories examples.
A dimensional reduction is then applied to have a significant
clusterization.  The recognition part is handled by a Bayesian classifier which
recognize a trajectory in joint or Cartesian space. The imitation is then
performed by interpolating known examples to obtain feasible trajectory.  This
method is not adapted for reactive behavior if the robot have a lot of degrees
of freedom or a large vocabulary of motion.
The introduction of Partially
Observable Markov Decision Process (POMDP) or Bayesian inference~\cite{pearl88} has
renewed the topic of action modeling~\cite{kaelbling98} in the last decade. Such
techniques and related ones are now applied to motor skill learning~\cite{peters08} in
general, and to motion segmentation~\cite{calinon10, inamura04} in particular. 

In~\cite{muhlig09} observations of a movement from sensors (camera, or motion capture system)
are mapped to a task space that will best represent and generalize the movement in order to focus
a learning technique into that particular space. The task space selection is
done by computing score functions inspired by neuroscience. The proposed criteria are
the saliency of the object that is manipulated, the variance of the dimension
of a space during several demonstrations, and some heuristics that express that
uncomfortable or exhausting motions are more relevant to a task. The method
presented add some higher level information to a purely statistical analysis. But
mapping an observation to a task space involves that all the important movements must
belong to that task space.\\

With respect to these statistics-based methods, our approach takes advantage of the
knowledge of the way the action is generated. It allows more powerful
disambiguation than pure statistic non-informed methods.\\

\section{Contribution}

In this paper we take another point of view: the control theory based
one. Control theory constitutes the second corpus that accompanies
robotics development. Originating from mechanics and applied
mathematics, it focuses on robot motion control~\cite{murray94,
siciliano10}. Among all the
contributions on linear and non-linear systems, robot control theory has
provided efficient concepts for motion generation. The research
initiated by A. Li\'egeois~\cite{liegeois77} on redundant robots (i.e. robots that have
more degrees of freedom than necessary to perform a given task), and
then developed by Y. Nakamura~\cite{nakamura91}, B. Siciliano, J.J. Slotine~\cite{siciliano91} and O.
Khatib~\cite{khatib87} introduce mathematical machinery based on linear algebra and
numerical optimization that allows for clever ways to model the symbolic
notion of \emph{task}~\cite{samson91}.\\

Those \emph{tasks} are defined by their tasks spaces (eg. position of the hand
in an arbitrary frame\cite{nakamura86a,khatib87b}, or position of a visual feature in the image
plane \cite{espiau92,hutchinson96a}), a reference behavior in those tasks spaces
(eg. exponential decrease to zero) and by the differential link between
the task space and the actuator space, typically the task Jacobian.
Given a set of active task, the corresponding control law can be
obtained by inverting the equation of motion of the robot. In this work
the control law is obtained by a \emph{stack of tasks}
(SoT)~\cite{mansard07} ie hierarchical inverse kinematics
\cite{siciliano91}.\\

The task function is generally used to generate a motion but
the originality of our approach to action recognition is to perform
reverse engineering on the stack of tasks. 
Knowing the set of all tasks that can appear in a motion and their associated behaviors, 
the motion that is supposed to be generated by a stack of tasks is processed in order to
seek the known behaviors.
The control law associated to a task function approach can be decomposed in two component:
a main command and a secondary command.
The secondary command is computed so as to not interfere with the main command. This is achieved by
projecting a secondary command into the nullspace of the main command. All degrees of freedom
that are not used by the main command will be used to achieve other commands.
Instead of using the projector onto the nullspace of a task to generate a new control law that takes into
account some lower priorities commands, the projector is used on a joint angle trajectory
to remove the part of the motion that correspond to a detected task, removing
ambiguity produced by combination of tasks.\\  

\section{Tasks and stack of tasks}
A task function~\cite{samson91} is an elegant approach to produce intuitively
sensor-based robot objectives. Based on the redundancy of the system, the
approach can be extended to consider a hierarchical set of
tasks~\cite{siciliano91}.  Defining the motion of the robot in terms of tasks
consists in choosing several control laws to be applied each on a different
subspace of the robot degrees of freedom. A task is defined by a space vector
$\mbf{e}$ (eg. the error between a signal $\mbf{s}$ and its desired value
$\mbf{e}=\mbf{s}^* - \mbf{s}$) and by reference behavior $\mbf{\dot{e}}$ to be
executed in the task space. The Jacobian of the task is noted
$\mbf{J}=\dpartial{\mbf{e}}{\mbf{q}}$ where $\mbf{q}$ is the robot
configuration vector.  We consider that the robot input control is the velocity
$\mbf{\dot{q}}$.  The control law is given by the least-square solution:
\begin{equation}
\dot{\mbf{q}} = \mbf{J}^+ \mbf{\dot{e}}^* + \mbf{P}\mbf{z}
\end{equation}

\noindent where $\mbf{J}^+$ is the least-square inverse of $\mbf{J}$,
$\mbf{P} = \mbf{I} - \mbf{J}^+ \mbf{J}$ is the null space
of $\mbf{J}$ and $\mbf{z}$ is any secondary criterion. $\mbf{P}$ ensures
a decoupling of the task with respect to $\mbf{z}$, which can be extended
recursively to a set of \emph{n} tasks. Those \emph{n} tasks
are ordered by priority : task number 1 being the highest priority task,
and task number \emph{n}, the lowest priority.
In other word, $task_i$ should not disturb $task_j$ if $i>j$.
The recursive formulation of the control law is proposed by~\cite{siciliano91} :
\begin{equation}
\dot{\mbf{q}}_i = \dot{\mbf{q}}_{i-1} + (\mbf{J}_i \mbf{P}_{i-1}^{A})^+
(\dot{\mbf{e}}^*_i - \mbf{J}_i \dot{\mbf{q}}_{i-1}) , \ \ i = 1 \ldots n
\end{equation}
\noindent with $\dot{\mbf{q}}_0 = 0$ and $\mbf{P}_{i-1}^{A}$ is
the projector onto the null space of the augmented Jacobian
$\mbf{J}_i^A = (\mbf{J}_1, \ldots \mbf{J}_i)$. The robot
joint velocity realizing all the tasks is $\dot{\mbf{q}}^* = \dot{\mbf{q}}_n$.
A complete implementation of this approach is proposed in~\cite{mansard07} under the
name \emph{Stack of Tasks} (SoT). 

The recursive formula proposed by~\cite{baerlocher98} is used to compute
the null space projector:
\begin{equation}
  \left\{
      \begin{array}{rl}
        \mbf{P}_0 &= \mbf{I}\\
        \mbf{P}_i &= \mbf{P}_{i-1} - (\mbf{J}_i \mbf{P}_{i-1})^+(\mbf{J}_i \mbf{P}_{i-1})\\
      \end{array}
    \right.
\end{equation}

\noindent where $\mbf{I}$ is the identity matrix, $\mbf{J}_i$ is the Jacobian matrix
of the task $i$. In this work, the projector is used on
the joint angle trajectory to cancel the sub-part of the robot involved in the task.

\section{Reverse engineering the stack of tasks} \label{sec:detect}

The input of the algorithm is the joint trajectories, and a set of candidate
tasks. The algorithm is iterative: at each iteration, the task that seems the
most relevant is selected and canceled in the original demonstration (Section~\ref{sec:alg2:proj}).
The algorithm finally stops when the original
motion is totally canceled by the successive task extraction. Deciding which
task is the most relevant is done by a curve fitting score. 

\subsection{Hypothesis}
It is assumed that the model behavior of a task is known, so the distance between
the theoretical and the actual trajectory represent how much a trajectory
was generated by a controllers (Section~\ref{sec:alg2:proj}).
The model of the robot and all tasks that may appear in a motion (the tasks pool) are known.
It is also assumed that all the tasks involved in a motion are compatible:
each tasks are completely fullfilled. As a consequence, all
tasks have the same priority order. 

\subsection{Task selection algorithm} \label{sec:alg1:selec}
The observed motion is generated by a SoT.  The idea is to identify which of the possible controllers are in the stack of tasks. 
In order to identify all the
tasks being performed in the observed motion, an iterative method is used where
the null space projectors have a central role, the projection
are used to decouple the tasks.
Each iteration consists in first, extracting the most significant task in the motion,
and then, removing the effect of this task using the properties of the null space projector.

The aim of the task selection algorithm is to reconstruct the set of tasks corresponding
to the observed motion, and the corresponding parameters.
The tasks are iteratively selected until it can be considered that all the tasks
have been detected.
This stop criterion is based on the
null space projection: if the projection of a motion
leads to a null motion, then no more tasks are involved.

Each time a task is selected, the observed motion is projected
onto the null space of that task, canceling its relative motion. 
The projected motion is then used to select another task.
The task selection algorithm is shown below :

\newcommand{\shOUTPUT}{\textbf{Output: }}
\newcommand{\shINPUT}{\textbf{Input: }}

\begin{algorithm}
  \caption{Task selection algorithm}
  \label{alg:taskSelection}
\begin{algorithmic}[1]
\STATE \shINPUT $\dot{q}^{*}(t)$
\STATE \shOUTPUT $activePool$
\STATE $P\dot{q}(t)\gets \dot{q}^{*}(t)$
\WHILE{$\int \Vert P\dot{q}(t) \Vert ^2 dt > \epsilon$ }
  \FOR{task $i = 1..n$}
    \STATE $r_i \gets \mathrm{taskFitting}(i)$
  \ENDFOR
  \STATE $i_{select} \gets \mathrm{argmin}(r_i)$
  \STATE $activePool.\mathrm{push}(i_{select})$
  \STATE $P\dot{q}(t) \gets \mathrm{projection}(i_{select}, P\dot{q}(t))$
\ENDWHILE
\end{algorithmic}
\end{algorithm}
where  $\dot{q}^{*}(t)$ is the joint angles velocities trajectory of the motion to analyze,
$P$ is a projector, $\dot{q}(t)$ the angle velocity trajectory
of the reference motion, $r_i$ is the score of the cost function of the optimization, $activePool$
the set of tasks selected during the algorithm, $\epsilon$ is the threshold
of the motion norm below which the algorithm stops. The task fitting is described
in the following section. The procedure $\mathrm{projection}(i, \dot{q}(t))$ compute the projector onto
the null space of the task $i$ and apply the projection to the motion.

The stop criterion will allow the discrimination of very close motions as it is illustrated in
the section~\ref{sec:distinc}.

\subsection{Task fitting by optimization} \label{sec:alg2:proj}
The point of this step is to quantify the relevance of a given task with respect to
the execution of a motion. This is achieved by applying a least-square optimization
between the reference motion and the reference behavior of a task over the parameters
of the reference behavior :
\begin{equation}
	\mbf{x}^* = \underset{\mbf{x}}\argmin \frac{\Vert \mbf{p}^*(t) - \mbf{p}_\mbf{x}(t) \Vert^2}{\Vert \mbf{p}^*(t) \Vert^2}
\label{optimProblem}
\end{equation}

\noindent where $\mbf{p}^*(t)$ is the reference trajectory, $\mbf{p}_\mbf{x}(t)$ is the trajectory
generated by the model using the parameters $\mbf{x}$. The CFSQP solver has been used in this work~\cite{lawrence97}.

\section{Results on simulation}
The demonstrations used in the experiments are generated by using the model
of the humanoid robot HRP-2 having 30 actuated degrees of freedom (plus 6 degrees of freedom on the
free-flyer). All reference motions start from the half-sitting pose showed in Fig.~\ref{fig:halfSit} and
the left foot is fixed to the ground.

The aim of those experiments is to illustrate the ability of the
task selection algorithm to distinguish two similar looking reference motions.

The set of tasks considered in those experiments are :

\begin{itemize}
\item Com : the center of mass of the robot is constraint to maintain the static balance
\item Gaze : the robot looks at one point in the Cartesian space
\item Twofeet: constraint the feet to stay both flat on the ground
\item Grab : is a reaching task, either the left or right hand
of the robot reaches a point defined in the Cartesian space
\item Screw : is similar to the grab task, but the desired position
has to be reached with a defined orientation.
\end{itemize}
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.3\linewidth]{img/halfSit.ps}
\end{center}
\caption{All reference motions start from a half-sitting pose.}
\label{fig:halfSit}
\end{figure}

\subsection{Application to the exponential decrease}
A reference behavior $\mbf{\dot{e}}$ can, for example, be defined by an exponential regulation.
For the humanoid robot, the characteristic trajectory in the task
space is modeled by an
exponential decrease with 3 input parameters :

\begin{equation}
\mbf p_{\mbf x}(t) = x_1 \mathrm{e}^{(-x_2 t)} + x_3
\label{eqTask}
\end{equation}

This is motivated by the nature of the control of the
robot.\\

\subsection{Experiment 1 : The projection of the motion}
In this experiment, a composed motion is generated, then the recognition by fitting
and the termination condition of the algorithm are validated.
The reference motion is composed of a right and left hand grabbing tasks,
the \emph{gaze} task, the \emph{Com} task and the \emph{Twofeet} task.

First a motion involving those tasks is generated in simulation.
The motion is given to the detection program
which select the tasks that are fitted the best by the optimization.

Fig.~\ref{fig:snapshotXpqdot} shows samples of posture during the original motion,
and the motions after successive projections
in the null spaces of the detected
tasks : \emph{right grab}, \emph{Com}, \emph{gaze}, \emph{Twofeet} and \emph{left grab}.
\begin{figure*}[t]
\centering
\begin{tabular}{c@{}c@{}c@{}c@{}c@{}c@{}c}
(a)&
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot0_0.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot0_99.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot0_199.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot0_299.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot0_399.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot0_499.png.ps}}\\

(b)&
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot1_0.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot1_99.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot1_199.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot1_299.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot1_399.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot1_499.png.ps}}\\

(c)&
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot2_0.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot2_99.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot2_199.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot2_299.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot2_399.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot2_499.png.ps}}\\

(d)&
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot3_0.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot3_99.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot3_199.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot3_299.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot3_399.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot3_499.png.ps}}\\

(e)&
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot4_0.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot4_99.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot4_199.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot4_299.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot4_399.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot4_499.png.ps}}\\

(f)&
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot5_0.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot5_99.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot5_199.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot5_299.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot5_399.png.ps}} &
\parbox[c]{2.4cm}{\includegraphics[width=\linewidth]{img/Pqdot5_499.png.ps}}\\
\\
Iteration & 0 & 99 & 199 & 299 & 399 & 499\\
\end{tabular}
\caption{Illustration of the successive projections of the motion (a) in the null spaces of the tasks,
(b) Right grab, (c) Com, (d) Gaze, (e) Twofeet, (f) Left grab.}
\label{fig:snapshotXpqdot}
\end{figure*}
Each projection cancels a part of the motion, and the robot's motion becomes null when all
projections are applied,
which means that all relevant tasks have been detected.
Fig.~\ref{fig:xp3Pqdot} shows the evolution of the norm of the motion,
defined by the sum square of the joint-angle velocity of the robot.
\begin{figure}[t]
\begin{center}
%\includegraphics[height=0.9\linewidth, angle = -90]{img/PqdotNorms.ps}
\resizebox{.48\textwidth}{!} {
      \input{img/PqdotNorms.pstex_t}
    }
\end{center}
\caption{Evolution of the norm of the motion after successively projecting the motion in the
	tasks null spaces.}
\label{fig:xp3Pqdot}
\end{figure}

\subsection{Experiment 2 : Distinction between two close motions}
\label{sec:distinc}
An interesting challenge in motion detection,
is to make the distinction between two motions
involving different tasks but still \emph{look} close.

\subsubsection{Reaching motions}
\label{sec:distinc1}
In this section, two motions are considered. Those motion are build from two different instances of stack of tasks.
The first one is a far-reaching task motion with the right hand.
The reaching motion of the right hand has an influence on the left hand.
Since this far-reaching task involves a large compensation of the center of mass
using the left hand.
In the remainder of the section, that motion will be called the
\emph{right-only hand motion}.
The second motion is the same reaching task motion for the right hand, added with a
second reaching task on the left hand. The desired position of the left
hand is artificially set to the final position of the left hand obtained at the first motion.
In the remainder of the section, that motion will be called the
\emph{both hands motion}.

A video showing those two motions on the HRP-2 is available\footnote{{http://homepages.laas.fr/shak/videos/}}.
The final states of the robot for the two motions are showed in the Fig.~\ref{fig:introExample:spotDiff}.
The two motions looks very similar, and it is, as shown on the video, very difficult
to tell which motion involves a left and right hand task without the context.
In the first case, the motion of the left hand is part of the \emph{com} task.
motion, as it is a side effect of the motion of the right hand.
In the second case, the motion of the left hand is independant,
because the left hand has been driven by its own goal.
However, in the task space, those motions are very different.
Fig.~\ref{fig:XP2RFit} shows the result of the task fitting for the \emph{right-only} motion.
The residue $r = \int{\frac{\Vert p^*(t) - p(t) \Vert^2}{\Vert p^*(t) \Vert^2} \mathrm{dt}}$ of the optimization shows that the task fitting performs poorly for the left hand task
compared to the fitting on the right hand.
\begin{figure*}[t]
\centering
\begin{tabular*}{0.9\textwidth}{@{\extracolsep{\fill}}cc}
%\includegraphics[width=0.9\linewidth]{img/projRHNull.eps}
  \resizebox{.4\textwidth}{!} {
      \input{img/rightHandXP2R.pstex_t}
    }          &
  \resizebox{.4\textwidth}{!} {
      \input{img/leftHandXP2R.pstex_t}
    }\\
  $r = 0.0106376 $  & $r = 0.273956$\\
\end{tabular*}
\caption{Task fitting on the \emph{right-only} hand motion on the right and the left hand tasks.
The residues $r$ of the optimizations are showed in the second row.}
\label{fig:XP2RFit}
\end{figure*}
Whereas Fig.~\ref{fig:XP2RLFit} shows the task fitting for the \emph{both hand} motion.
\begin{figure*}[t]
\centering
\begin{tabular*}{0.9\textwidth}{@{\extracolsep{\fill}}cc}
%\includegraphics[width=0.9\linewidth]{img/projRHNull.eps}
  \resizebox{.4\textwidth}{!} {
      \input{img/rightHandXP2RL.pstex_t}
    }                           &
  \resizebox{.4\textwidth}{!} {
      \input{img/leftHandXP2RL.pstex_t}
    }\\
  $r = 0.0138049 $ & $r = 0.0244404$\\
\end{tabular*}
\caption{Task fitting on the \emph{both} hand motion on the right and the left hand tasks.
The residues $r$ of the optimizations are showed in the second row.}
\label{fig:XP2RLFit}
\end{figure*}
The results of the detection algorithm
applied to those motion are showed in Table~\ref{tab:spotDiff1}.

\begin{table}[t]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Reference & Detected & $\int \Vert \dot{q}(t) \Vert ^2 dt$ & $\int \Vert P\dot{q}(t) \Vert ^2 dt$ \\
\hline
\begin{tabular}{c}
Com\\
Right Hand\\
Twofeet\\
\end{tabular}

&

\begin{tabular}{c}
Com\\
Right Hand\\
Twofeet\\
\end{tabular}

& 0.364398 & 0.00159355 \\
\hline
\begin{tabular}{c}
Com\\
Left Hand\\
Right Hand\\
Twofeet\\
\end{tabular}

&
\begin{tabular}{c}
Com\\
Left Hand\\
Right Hand\\
Twofeet\\
\end{tabular}

& 0.538329  & 0.0035343 \\
\hline
\end{tabular}
\caption{Results of the task selection algorithm from the analysis of the \emph{right-only} motion and the \emph{both hands} motion.}
\label{tab:spotDiff1}
\vspace{-20pt}
\end{table}
The first column shows the
tasks being used in the reference motion, the second columns shows the tasks
selected by the algorithm, the third column shows the norm of the reference motion,
and the last column shows the norm of the reference motion projected onto the null space of
all the selected tasks. The threshold for the stop criterion $\int \Vert P\dot{q}(t) \Vert ^2 dt > \epsilon$
is $\epsilon = 0.1$.
It has to be noted that the tasks involved in the reference motions are completely fulfilled.
This means that the priority order of each tasks has no influence on the motion.
The projection of the motion in the null space of the right \emph{grab} and \emph{Com} task
will have different consequences for the \emph{right-only} and \emph{both hands}
motion. Projecting the \emph{right-only} motion into the null spaces of
the right \emph{grab} and the \emph{Com} will cancel all the motion involved in those tasks, including
the \emph{side-effect} motion of the left hand. The norm
of the remaining motion is therefore small, as the task error for the left hand becomes very small.
Fig.~\ref{fig:RbeforeAfterProj} shows the right and left hand tasks errors in the \emph{right-only} hand motion (before projection),
and after projection in the null space of the right hand and the \emph{Com} tasks).
\begin{figure*}[t]
\centering
\begin{tabular*}{0.9\textwidth}{@{\extracolsep{\fill}}cc}
    \resizebox{.4\textwidth}{!} {
      \input{img/RbeforeProj.pstex_t}
    }&
    \resizebox{.4\textwidth}{!} {
      \input{img/RafterProj.pstex_t}
    }\\
Before projection & After projection\\
\end{tabular*}
\caption{Task error for the right and left hand tasks in the \emph{right-only} hand motion,
before and after the projection onto the null space of the \emph{right hand} task and \emph{Com} task.}
\label{fig:RbeforeAfterProj}
\end{figure*}

On the other hand, when projecting the \emph{both hands} motion, the remaining motion
after projection is not null. The remaining motion is the motion involved by the left hand task, and therefore
its norm is still significant. The algorithm will continue and search for this task.
Fig.~\ref{fig:RLbeforeAfterProj} shows the right and left hand tasks errors in the \emph{both} hands motion (before projection),
and after projection in the null space of the right hand and the \emph{Com} tasks).
\begin{figure*}[t]
\centering
\begin{tabular*}{0.9\textwidth}{@{\extracolsep{\fill}}cc}
    \resizebox{.4\textwidth}{!} {
      \input{img/RLbeforeProj.pstex_t}
    }&
    \resizebox{.4\textwidth}{!} {
      \input{img/RLafterProj.pstex_t}
    }\\
Before projection & After projection\\
\end{tabular*}
\caption{Task error for the right and left hand tasks in the \emph{both} hands motion,
before and after the projection onto the null space of the \emph{right hand} task and \emph{Com} task.}
\label{fig:RLbeforeAfterProj}
\end{figure*}
\subsubsection{Grab and screwing motions}
\label{sec:distinc2}
In this section, the two motions considered are the \emph{grabbing} and the \emph{screwing}
motion.

In both motions, other tasks are added : the task \emph{Twofeet}, the task \emph{Com} constraints,
the task \emph{gaze}.
Both \emph{grabbing} and \emph{screwing} motions share the same position target. The only difference between
the two demonstrated motions is the
presence of an orientation constraint for the right hand in the \emph{screwing} motion.
The final positions of those motions are showed in the Fig.~\ref{fig:spotDiff2}.
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.55\linewidth]{img/spotDiff2bis.ps}
\end{center}
\caption{Left: The final position of the grab motion; Right: The final position of the screwing motion.}
\label{fig:spotDiff2}
\vspace{-3pt}
\end{figure}
Table~\ref{tab:spotDiff2} shows the results of the tasks selection algorithm applied
to the \emph{grabbing} and \emph{screwing motions}.
\begin{table}[t]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Reference & Detected & $\int \Vert \dot{q}(t) \Vert ^2 dt$ & $\int \Vert P\dot{q}(t) \Vert ^2 dt$ \\
\hline
\begin{tabular}{c}
Com\\
Gaze\\
Grab\\
Twofeet\\
\end{tabular}
&
\begin{tabular}{c}
Com\\
Gaze\\
Grab\\
Twofeet\\
\end{tabular}
& 0.703409 & 0.0022444 \\
\hline
\begin{tabular}{c}
Com\\
Screw\\
Twofeet\\
\end{tabular}
&
\begin{tabular}{c}
Com\\
Grab\\
Screw\\
Twofeet\\
\end{tabular}
& 0.825217 & 0.00460314\\
\hline
\end{tabular}
\caption{Results of the task selection algorithm from the analysis of the
  \emph{grabbing} motion and the \emph{screwing} motion.}
\label{tab:spotDiff2}
\vspace{-27pt}
\end{table}
During the analysis of the \emph{screwing} motion, the \emph{grabbing} task
is selected as well since the grab task is
actually a sub-task of the \emph{screwing} motion. Of course, the \emph{screwing} task is not selected
in the analysis of the grab task.

\subsubsection{Screw and gaze motions}
\label{sec:distinc3}
The two motions considered are a \emph{gaze} motion, and a \emph{screw} motion. An object
is in front of the robot, and creates an occlusion in the vision of the robot.
To get rid of this occlusion, the robot can lean on his right. That is the \emph{gaze}
motion considered. While the robots leans, the left hand is dragged by the chest.
The position and orientation of that hand is recorded as a desired state for the first \emph{screw motion} task.
The final states of the robot are showed in Fig.~\ref{fig:spotDiff3}
and the results of the analysis are summarized in Table~\ref{tab:spotDiff3}.
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.66\linewidth]{img/spotDiff3.ps}
\end{center}
\caption{Left: The final position of the gaze motion; Right: The final position of the grab motion.}
\label{fig:spotDiff3}
\end{figure}
\begin{table}[t]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Reference & Detected & $\int \Vert \dot{q}(t) \Vert ^2 dt$ & $\int \Vert P\dot{q}(t) \Vert ^2 dt$ \\
\hline
\begin{tabular}{c}
Com\\
Gaze\\
Twofeet\\
\end{tabular}

&

\begin{tabular}{c}
Com\\
Gaze\\
Twofeet\\
\end{tabular}

& 0.534478 & 0.00165744 \\
\hline
\begin{tabular}{c}
Com\\
Lhand\\
Twofeet\\
\end{tabular}

&
\begin{tabular}{c}
Com\\
Lhand\\
Twofeet\\
\end{tabular}

& 0.558084 & 0.00239261\\
\hline
\end{tabular}
\caption{Results of the task selection algorithm from the analysis of the grab motion and the gaze motion.}
\label{tab:spotDiff3}
\vspace{-12pt}
\end{table}

\section{Experimentation on the robot}

\subsection{Experimental setup}
A motion generated with the Stack of Tasks is executed on the Robot HRP-2 and
a motion capture system provides the trajectory of each body of the robot in order 
to have an external observation of the robot.
The motion capture system used is composed of 10 digital cameras and record data
at 200Hz (see Fig.~\ref{fig:camera}). 
\begin{figure}[t]
  \centering
  \includegraphics[height=0.4\linewidth]{img/camIR.ps} 
  \caption{Camera of the motion capture system.}
  \label{fig:camera}
\end{figure}

A set of 50 markers
are placed on the robot (Fig.~\ref{fig:hrp2Markers}), and the data collected from those markers
are used to build a virtual skeleton that match the kinematic structure of the robot.
\begin{figure}[t]
  \centering
  \begin{tabular}{cc}
    \includegraphics[height=0.7\linewidth]{img/hrp2Markers.ps} &
    \includegraphics[height=0.7\linewidth]{img/skel.ps} \\
  \end{tabular}
  \caption{Markers set and virtual skeleton of HRP2.}
  \label{fig:hrp2Markers}
\end{figure}

The reverse engineering is performed on the joint angle trajectory of the robot.
So in order to estimate them, a mapping between
the kinematic model of the robot and the kinematic model of the virtual skeleton
has to be made. Then the joint angle that would bring the virtual skeleton to the same posture
as the actual robot is the joint angle that minimize the distance between
the transformation from the measured skeleton kinematic transformation and the transformation in the robot kinematic model.
\begin{equation}
  \mbf{q}^*_i = \underset{\mbf{q}_i}\argmin \Vert K_W\tensor[^{0M}]{R}{_{iM}}K_i \ominus \tensor[^W]{R}{_{iQ}}(\mbf{q}_i) \Vert ^2\\
  \label{mocapOpti}
\end{equation}
where $\ominus$ is the distance operator, $K_W$ is the transformation matrix from the origin of the reference of the robot model to the reference of the 
motion capture data. $\tensor[^{0M}]{R}{_{iM}}$ is the transformation matrix from the root of the virtual bone to
the bone $i$ in the motion capture reference. $K_i$ is the transformation matrix from the virtual bone $i$ to
the corresponding joint of the robot model and $\tensor[^W]{R}{_{iQ}}$ is the transformation matrix from 
the origin of the reference of the robot model to the joint $i$ in the robot model reference.
$K_W$ and $K_i$ are constant and are computed in a calibration step where both real joint angle and
measured skeleton are known. The joint angle trajectory is then used for the reverse engineering analysis.\\

On the real robot, the flexibility of the ankles interfers with the motion at the very beginning as the acceleration
of the joint angle increase quickly. The flexibility is not modeled in our algorithm, so
in order to bypass the influence of the flexibility, the motion analysis is done after the first 100ms.

\subsection{Right hand VS Both hands motion}
Table~\ref{tab:spotDiffReal1} shows the result of the identification algorithm run on
the motion data of the \emph{right hand} and \emph{both hands} motion.
\begin{table}[t]
  \centering
  \begin{tabular}{|c|c|c|c|}
    \hline
    Reference & Detected & $\int \Vert \dot{q}(t) \Vert ^2 dt$ & $\int \Vert P\dot{q}(t) \Vert ^2 dt$ \\
    \hline
    \begin{tabular}{c}
      Com\\
      Right Hand\\
      Twofeet\\
    \end{tabular}

    &

    \begin{tabular}{c}
      Com\\
      Right Hand\\
      Twofeet\\
    \end{tabular}

    & 0.104835 & 0.0482885 \\
    \hline
    \begin{tabular}{c}
      Com\\
      Left Hand\\
      Right Hand\\
      Twofeet\\
    \end{tabular}

    &
    \begin{tabular}{c}
      Com\\
      Left Hand\\
      Right Hand\\
      Twofeet\\
    \end{tabular}

    & 0.142293  & 0.0541836 \\
    \hline
  \end{tabular}
  \caption{Results of the task selection algorithm from the analysis of the \emph{right-only} motion and the \emph{both hands} motion on the real robot.}
  \label{tab:spotDiffReal1}
  \vspace{-20pt}
\end{table}
Fig.~\ref{fig:exp1:taskCom0}-\ref{fig:exp1:taskTwofeet0} shows the
fitting at the first iteration of the algorithm. 
For the \emph{right hand} motion, the projection in the task of the left hand projection have a poor fitting
(Fig.~\ref{fig:exp1:taskLhand0:R}) with the model of the task, whereas the right hand task have a good
fit (Fig.~\ref{fig:exp1:taskRhand0:R}).

%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}[t]
  \centering
  \subfigure[Right hand motion - iteration 0]{
  \resizebox{.48\textwidth}{!} {
  \input{img/exp1/R/invJerror_taskCom_0}
  }
  \label{fig:exp1:taskCom0:R}
  }
  \subfigure[Both hands motion - iteration 0]{
  \resizebox{.48\textwidth}{!} {
  \input{img/exp1/RL/invJerror_taskCom_0}
  }
  \label{fig:exp1:taskCom0:RL}
  }
  \caption{Comparison of the fitting for the center of masses regulation task for the \emph{both hands} and \emph{right hand} motion at the first iteration of the identification algorithm.
  $r$ are the residues of the optimizations.}
  \label{fig:exp1:taskCom0}
\end{figure*}

Notice that in the \emph{both hands} motion, the candidate head task involves less motion
than in the \emph{right hand} motion. The reason is that when the opposite end effectors
of the robot are constrained, the chest cannot be used to execute the head task.

\begin{figure*}[t]
  \centering
  \subfigure[Right hand motion - iteration 0]{
  \resizebox{.48\textwidth}{!} {
  \input{img/exp1/R/invJerror_taskHead_0}
  }                           
  \label{fig:exp1:taskHead0:R}
  }
  \subfigure[Both hands motion - iteration 0]{
  \resizebox{.48\textwidth}{!} {
  \input{img/exp1/RL/invJerror_taskHead_0}
  }
  \label{fig:exp1:taskHead0:RL}
  }
  \caption{Comparison of the fitting for the head task for the
  \emph{both hands} and \emph{right hand} motion at the first
  iteration of the identification algorithm.  $r$ are
  the residues of the optimizations.}
  \label{fig:exp1:taskHead0}
\end{figure*}

\begin{figure*}[t]
  \centering
  \subfigure[Right hand motion - iteration 0]{
  \resizebox{.48\textwidth}{!} {
  \input{img/exp1/R/invJerror_taskLhand_0}
  }
  \label{fig:exp1:taskLhand0:R}
  }
  \subfigure[Both hands motion - iteration 0]{
  \resizebox{.48\textwidth}{!} {
  \input{img/exp1/RL/invJerror_taskLhand_0}
  }
  \label{fig:exp1:taskLhand0:RL}
  }
  \caption{Comparison of the fitting for the left hand task for the
  \emph{both hands} and \emph{right hand} motion at the first
  iteration of the identification algorithm.  $r$ are
  the residues of the optimizations.}
  \label{fig:exp1:taskLhand0}
\end{figure*}

\begin{figure*}[t]
  \centering
  \subfigure[Right hand motion - iteration 0]{
  \resizebox{.48\textwidth}{!} {
  \input{img/exp1/R/invJerror_taskRhand_0}
  }
  \label{fig:exp1:taskRhand0:R}
  }
  \subfigure[Both hands motion - iteration 0]{
  \resizebox{.48\textwidth}{!} {
  \input{img/exp1/RL/invJerror_taskRhand_0}
  }
  \label{fig:exp1:taskRhand0:RL}
  }
  \caption{Comparison of the fitting for the right hand task for the
  \emph{both hands} and \emph{right hand} motion at the first
  iteration of the identification algorithm.  $r$ are
  the residues of the optimizations.}
  \label{fig:exp1:taskRhand0}
\end{figure*}

\begin{figure*}[t]
  \centering
  \subfigure[Right hand motion - iteration 0]{
  \resizebox{.48\textwidth}{!} {
  \input{img/exp1/R/invJerror_taskTwofeet_0}
  }
  \label{fig:exp1:taskTwofeet0:R}
  }
  \subfigure[Both hands motion - iteration 0]{
  \resizebox{.48\textwidth}{!} {
  \input{img/exp1/RL/invJerror_taskTwofeet_0}
  }
  \label{fig:exp1:taskTwofeet0:RL}
  }
  \caption{Comparison of the fitting for the \emph{two feet} task for the
  \emph{both hands} and \emph{right hand} motion at the first
  iteration of the identification algorithm.  $r$ are the
  residues of the optimizations.}
  \label{fig:exp1:taskTwofeet0}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
Fig.~\ref{fig:exp1:taskCom1}-\ref{fig:exp1:taskTwofeet1} show how the
tasks fitting evolves after the extraction of one task.
The right hand task has been automatically selected and removed in both cases.
It is therefore the second iteration of the algorithm. 
The motion remaining from the extraction of that task is then used for the next iteration.

\begin{figure*}[t]
  \centering
  \subfigure[Right hand motion - iteration 1]{
  \resizebox{.48\textwidth}{!} {
	\input{img/exp1/R/invJerror_taskCom_1}
    }
    \label{fig:exp1:taskCom1:R}
  }
  \subfigure[Both hands motion - iteration 1]{
  \resizebox{.48\textwidth}{!} {
	\input{img/exp1/RL/invJerror_taskCom_1}
    }
    \label{fig:exp1:taskCom1:RL}
  }
\caption{Comparison of the fitting for the center of masses regulation task for the \emph{both hands} and \emph{right hand} motion at the second iteration of the identification algorithm.
$r$ are the residues of the optimizations.}
\label{fig:exp1:taskCom1}
\end{figure*}

\begin{figure*}[t]
  \centering
  \subfigure[Right hand motion - iteration 1]{
  \resizebox{.48\textwidth}{!} {
	\input{img/exp1/R/invJerror_taskHead_1}
    } 
    \label{fig:exp1:taskHead1:R}
  }
  \subfigure[Both hands motion - iteration 1]{
  \resizebox{.48\textwidth}{!} {
	\input{img/exp1/RL/invJerror_taskHead_1}
    }
    \label{fig:exp1:taskHead1:RL}
  }
\caption{Comparison of the fitting for the head task for the \emph{both hands} and \emph{right hand} motion at the second iteration of the identification algorithm.
$r$ are the residues of the optimizations.}
\label{fig:exp1:taskHead1}
\end{figure*}

\begin{figure*}[t]
  \centering
  \subfigure[Right hand motion - iteration 1]{
    \resizebox{.48\textwidth}{!} {
      \input{img/exp1/R/invJerror_taskLhand_1}
    }
    \label{fig:exp1:taskLhand1:R}
  }
  \subfigure[Both hands motion - iteration 1]{
    \resizebox{.48\textwidth}{!} {
      \input{img/exp1/RL/invJerror_taskLhand_1}
    }
    \label{fig:exp1:taskLhand1:RL}
  }
  \caption{Comparison of the fitting for the left hand task for the
    \emph{both hands} and \emph{right hand} motion at the second
    iteration of the identification algorithm.  $r$ are
    the residues of the optimizations.}
  \label{fig:exp1:taskLhand1}
\end{figure*}

\begin{figure*}[t]
  \centering
  \subfigure[Right hand motion - iteration 1]{
  \resizebox{.48\textwidth}{!} {
	\input{img/exp1/R/invJerror_taskTwofeet_1}
    }
    \label{fig:exp1:taskTwofeet1:R}
  }
  \subfigure[Both hands motion - iteration 1]{
  \resizebox{.48\textwidth}{!} {
	\input{img/exp1/RL/invJerror_taskTwofeet_1}
    }
    \label{fig:exp1:taskTwofeet1:RL}
  }
\caption{Comparison of the fitting for the \emph{two feet} task for the \emph{both hands} and \emph{right hand} motion at the second iteration of the identification algorithm. $r$ are the residues of the optimizations.}
\label{fig:exp1:taskTwofeet1}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan

%\begin{figure*}[t]
%  \centering
%  \subfigure[Right hand motion - iteration 2]{
%  \resizebox{.48\textwidth}{!} {
%	\input{img/exp1/R/invJerror_taskHead_2}
%    } 
%    \label{fig:exp1:taskHead2:R}
%  }
%  \subfigure[Both hands motion - iteration 2]{
%  \resizebox{.48\textwidth}{!} {
%	\input{img/exp1/RL/invJerror_taskHead_2}
%    }
%    \label{fig:exp1:taskHead2:RL}
%  }
%\caption{Comparison of the fitting for the head task for the \emph{both hands} and \emph{right hand} motion at the third iteration of the identification algorithm.
%$r$ are the residues of the optimizations.}
%\label{fig:exp1:taskHead2}
%\end{figure*}
%
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan
%
%\begin{figure*}[t]
%  \centering
%  \subfigure[Right hand motion - iteration 2]{
%  \resizebox{.48\textwidth}{!} {
%	\input{img/exp1/R/invJerror_taskLhand_2}
%    } 
%    \label{fig:exp1:taskLhand2:R}
%  }
%  \subfigure[Both hands motion - iteration 2]{
%  \resizebox{.48\textwidth}{!} {
%	\input{img/exp1/RL/invJerror_taskLhand_2}
%    }
%    \label{fig:exp1:taskLhand2:RL}
%  }
%\caption{Comparison of the fitting for the left hand task for the \emph{both hands} and \emph{right hand} motion at the third iteration of the identification algorithm.
%$r$ are the residues of the optimizations.}
%\label{fig:exp1:taskLhand2}
%\end{figure*}
%
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan
%
%\begin{figure}[t]
%  \centering
%  %\subfigure[Both hands motion - iteration 2]{
%  \resizebox{.48\textwidth}{!} {
%  \begin{tabular}{c}
%	\input{img/exp1/R/invJerror_taskCom_2}\\
%  Right hand motion - iteration 2
%  \end{tabular}
%  }
%  %\label{fig:exp1:taskCom2:R}
%  %}
%\caption{Fitting for the com task for the \emph{right hand} motion at the third iteration of the identification algorithm.
%$r$ is the residue of the optimization.}
%\label{fig:exp1:taskCom2}
%\end{figure}
%
%satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan
%
%\begin{figure}[t]
%  \centering
%  %\subfigure[Both hands motion - iteration 2]{
%  \resizebox{.48\textwidth}{!} {
%  \begin{tabular}{c}
%	\input{img/exp1/RL/invJerror_taskTwofeet_2}\\
%  Both hands motion - iteration 2
%  \end{tabular}
%    }
%  %\label{fig:exp1:taskTwofweet2:RL}
%  %}
%\caption{Fitting for the twofeet task for the \emph{both hands} motion at the third iteration of the identification algorithm.
%$r$ is the residue of the optimization.}
%\label{fig:exp1:taskTwofeet2}
%\end{figure}
%
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure*}[t]
%  %\subfigure[Both hands motion - iteration 3]{
%  \resizebox{.48\textwidth}{!} {
%  \begin{tabular}{c}
%	\input{img/exp1/RL/invJerror_taskHead_3}\\
%  Both hands motion - iteration 3
%  \end{tabular}
%    }
%    %\label{fig:exp1:taskHead3:RL}
%  %}
%\caption{Comparison of the fitting for the head task for the \emph{both hands} motion at the fourth iteration of the identification algorithm.
%$r$ is the residue of the optimization.}
%\label{fig:exp1:taskHead3}
%\end{figure*}
%
%satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%
%\begin{figure}[t]
%  \centering
%  \subfigure[Both hands motion - iteration 3]{
%  \resizebox{.48\textwidth}{!} {
%        \input{img/exp1/RL/invJerror_taskLhand_3}
%    }
%    %\label{fig:exp1:taskLhand3:RL}
%  }
%\caption{Comparison of the fitting for the left hand task for the \emph{both hands} motion at the fourth iteration of the identification algorithm.
%$r$ are the residues of the optimizations.}
%\label{fig:exp1:taskLhand3}
%\end{figure}
%
%satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan

%\begin{figure*}[t]
%\centering
%\begin{tabular}{|c|c|}
%  \hline
%  Right hand motion & Both hands motion\\
%  \hline
%    \resizebox{.43\textwidth}{!}{
%      \input{img/exp1/R/invJerror_taskRhand_0}
%    }
%  &
%    \resizebox{.43\textwidth}{!}{
%      \input{img/exp1/RL/invJerror_taskRhand_0}
%    }\\
%  \hline
%\end{tabular}
%\caption{Right hand task fiting for \emph{right hand} and \emph{both hands} motion.}
%\end{figure*}
%
%satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan
%satan satan satan satan satan satan satan satan satan satan satan

\begin{figure*}[t]
\centering
\begin{tabular}{|c|c|}
  \hline
  Right hand motion & Both hands motion\\
  \hline
  \subfigure{
    \resizebox{.43\textwidth}{!}{
      \input{img/exp1/R/invJerror_taskLhand_0}
    }
  }
  &
  \subfigure{
    \resizebox{.43\textwidth}{!}{
      \input{img/exp1/RL/invJerror_taskLhand_0}
    }
  }\\
  \hline
  \subfigure{
    \resizebox{.43\textwidth}{!}{
      \input{img/exp1/R/invJerror_taskLhand_1}
    }
  }
  &
  \subfigure{
    \resizebox{.43\textwidth}{!}{
      \input{img/exp1/RL/invJerror_taskLhand_1}
    }
  }\\
  \hline
  \subfigure{
    \resizebox{.43\textwidth}{!}{
      \input{img/exp1/R/invJerror_taskLhand_2}
    } 
  }
  &
  \subfigure{
    \resizebox{.43\textwidth}{!}{
      \input{img/exp1/RL/invJerror_taskLhand_2}
    }
  }\\ 
  \hline
  &
  \subfigure{
    \resizebox{.43\textwidth}{!}{
      \input{img/exp1/RL/invJerror_taskLhand_3}
    }
  }\\
  \hline
\end{tabular}
\caption{Left hand task fitting in each iteration of the reverse engineering. The rows represent
the iteration, and the columns represent the motion.}
\end{figure*}

Fig.~\ref{fig:exp1:PqdotNormsR} shows the evolution of the norm of the motion after each task selection
on the \emph{right hand} motion. The order of the task extraction is :
\emph{right hand}, \emph{Twofeet} and \emph{com} task.
Whereas Fig.~\ref{fig:exp1:PqdotNormsRL} shows the evolution for the \emph{both hands} motion.
This time the extracted task are : \emph{right hand},  \emph{com}, \emph{Twofeet} and \emph{left hand}.

\begin{figure}[t]
  \resizebox{.48\textwidth}{!} {
    \input{img/exp1/R/PqdotNormsR}
  }
\caption{Evolution of the norm of the motion after successively projecting the motion in the tasks null space for
the \emph{right hand} motion.}
\label{fig:exp1:PqdotNormsR}
\end{figure}

\begin{figure}[t]
  \resizebox{.48\textwidth}{!} {
    \input{img/exp1/RL/PqdotNormsRL}
  }
\caption{Evolution of the norm of the motion after successively projecting the motion in the tasks null space for
the \emph{both hands} motion.}
\label{fig:exp1:PqdotNormsRL}
\end{figure}

\section{Conclusion}
The work presented in this article described a method to identify what tasks are involved in a movement.
The analysis is driven by the knowledge
of what is a task and how it behaves (in this article, the exponential decrease is used as an example of behavior). 
Furthermore, the analyzed movement is supposed to be generated by a set of 
controllers belonging to a known pool of controllers (the tasks). 
The task recognition problem is then tackled by a reverse engineering of the motion :
what controllers have been used in the observed motion?

The method has been successfully applied in different
scenarios to discriminate similar looking motions. Those motions were chosen to be especially
ambiguous in order to illustrate the efficiency of the method.

The method does not rely on the nature of the behavior of a task. Therefore
other control laws can be used. Future works involves the investigation of the use of 
a minimum jerk trajectory for the characterization of a task in order to match
a human reaching trajectories. 

\bibliographystyle{unsrt}
\bibliography{biblio}

\end{document}
